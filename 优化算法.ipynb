{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optimization</h1>\n",
    "\n",
    "# 0. 梯度下降 Gradient Descent\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "> $J(\\theta) = \\sum_{i=1}^{N}L(f(x_i;\\theta), y)$\n",
    "\n",
    "## 优化目标\n",
    "> $\\theta^{*} = \\arg\\min_{\\theta}J(\\theta)$\n",
    "\n",
    "## 梯度下降\n",
    "根据泰勒展开，\n",
    "\n",
    "> $f(x+\\epsilon) \\approx f(x) + \\epsilon f(x)$\n",
    "\n",
    "对于特别小的$\\epsilon$，\n",
    "\n",
    "> $f(x - \\epsilon * sign(f'(x)) \\approx f(x) - \\epsilon * sign (f'(x))$\n",
    "\n",
    "如果$\\epsilon$与$sign(f'(x))$同号，即$x$按照$sign(f'(x))$的反反向移动一小步$\\epsilon$，能让$f(x)$的值有所下降，\n",
    "\n",
    "> $f(x - \\epsilon * sign(f'(x)) < f(x)$\n",
    "\n",
    "### 局部最小值\n",
    "\n",
    "如果$f'(x) = 0$，导数不能提供函数从当前$x$移动方向的信息。\n",
    "\n",
    "如果对于比较小的$\\epsilon$，有$f(x-\\epsilon) < f(x) < f(x+\\epsilon)$，则$f(x)$是一个局部最小值。\n",
    "\n",
    "### 鞍点\n",
    "\n",
    "如果有$f'(x) = 0$，且$f(x)$既不是局部最大值，也不是局部最小值，则$x$为鞍点。\n",
    "\n",
    "## 最速下降\n",
    "\n",
    "对于多维输入$\\boldsymbol{x} = \\{x_1, x_2, ..., x_M\\}$，$f(\\boldsymbol{x})$关于$x_i$的偏导数为\n",
    "\n",
    "> $\\frac{\\partial}{\\partial{x_i}}f(\\boldsymbol{x})$\n",
    "\n",
    "衡量的是$x_i$的变化对函数值$f(\\boldsymbol{x})$的大小的影响。\n",
    "\n",
    "$f(\\boldsymbol{x})$的导数$\\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x})}$是所有关于$x_i$的偏导数的向量。\n",
    "\n",
    "### 方向偏导\n",
    "\n",
    "对于单位向量$\\boldsymbol{u}$的方向u，其方向偏导为$f(\\boldsymbol{x} + a \\boldsymbol{u})$在$a=0$关于方向u的偏导数。由链式法则\n",
    "\n",
    "> $ \\frac{\\partial} {\\partial{a}} f(\\boldsymbol{x} + a \\boldsymbol{u})\n",
    "= u^{\\top}\\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x} + a \\boldsymbol{u})}\n",
    "$\n",
    "\n",
    "从而，\n",
    "> $\\arg\\min_{\\boldsymbol{u}, \\boldsymbol{u}^{\\top}\\boldsymbol{u} = 1}\n",
    "u^{\\top}\\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x} + a \\boldsymbol{u})}$\n",
    "\n",
    "> $=||\\boldsymbol{u}||_2 ||\n",
    "\\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x} + a \\boldsymbol{u})}||_2 \n",
    "\\cos\\theta$\n",
    "\n",
    "$\\theta$为向量$\\boldsymbol{u}$与向量$\\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x} + a \\boldsymbol{u})}$的夹角。\n",
    "\n",
    "$||\\boldsymbol{u}|| = 1$，$\\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x} + a \\boldsymbol{u})}$与$\\boldsymbol{u}$无关，上式变为\n",
    "\n",
    "> $\\arg\\min_{\\boldsymbol{u}, \\boldsymbol{u}^{\\top}\\boldsymbol{u} = 1}\n",
    "\\cos\\theta$\n",
    "\n",
    "所以，$\\boldsymbol{u}$应选择与$\\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x} + a \\boldsymbol{u})}$相反的方向。\n",
    "\n",
    "为减小$f$，当前$\\boldsymbol{x}$应移动为\n",
    "\n",
    "> $\\boldsymbol{x'} = x - \\epsilon \\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x})}$。\n",
    "\n",
    "$\\epsilon>0$是学习率。可以选择固定的比较小的学习率，也可以选择\n",
    "\n",
    "### Line Search\n",
    "\n",
    "使得\n",
    "> $f(x -  \\epsilon \\nabla_{\\boldsymbol{x}}{f(\\boldsymbol{x})})$\n",
    "\n",
    "最小的$\\epsilon$。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dropout\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1] Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n",
    "\n",
    "\n",
    "神经网络：\n",
    "\n",
    "共$L$层，每一层输入$\\boldsymbol{z^{(l)}}$，输出$\\boldsymbol{y^{(l)}}$，激活函数$f^{(l)}$，每一层节点个数$Q(l)$， $l\\in \\{0,1,...,L\\}$\n",
    "\n",
    "第$l$层到第$l+1$层的权重为$\\boldsymbol{W^{(l)}}$，偏置为$\\boldsymbol{b}^{(l)}$，$l\\in \\{0,1,...,L-1\\}$\n",
    "\n",
    "## a. 第$l+1$层输出\n",
    "\n",
    "> $z_i^{(l+1)} = \\boldsymbol{w}_i^{(l)}\\boldsymbol{y}^{(l)} + \\boldsymbol{b}_i^{(l)}\n",
    "= \\sum_{j=1}^{Q(l)}\n",
    "\\{w_{ji}^{(l)}y_j^{(l)}+b_i^{(l)}\\}$\n",
    "\n",
    "> $y_i^{(l+1)} = f^{(l+1)}(z_i^{(l+1)})$\n",
    "\n",
    "\n",
    "## b. Dropout 第$l+1$层输出\n",
    "\n",
    "假设每个节点的保留概率为$p$，\n",
    "\n",
    "> $r_i^{(l)} \\sim Bernoulli(p)$\n",
    "\n",
    "> $\\widetilde{\\boldsymbol{y}}_i^{(l)} =\n",
    "r_i^{(l)} * \\widetilde{\\boldsymbol{y}}_i^{(l)}$\n",
    "\n",
    "> $z_i^{(l+1)} = \\boldsymbol{w}_i^{(l)} \\widetilde{\\boldsymbol{y}}_i^{(l)} \n",
    "+ \\boldsymbol{b}_i^{(l)}$\n",
    "\n",
    "> $y_i^{(l+1)} = f^{(l+1)}(z_i^{(l+1)})$\n",
    "\n",
    "### (1) 训练数据权重不变，测试数据权重变化\n",
    "\n",
    "训练数据权重不变，每个节点以概率$p$保留，测试数据权重\n",
    "\n",
    "> $W_{test} = W_{train} * p$\n",
    "\n",
    "### (2) 训练数据权重变化，测试数据权重不变\n",
    "\n",
    "> $W_{train} = W_{train} * \\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian矩阵\n",
    "\n",
    "如果$f(\\boldsymbol{x})$的输入、输出都是向量，那么其输出向量的每个元素$f(\\boldsymbol{x})_i$，关于输入向量的每个元素$x_j$的偏导是一个矩阵，被称为Jacobian矩阵。\n",
    "\n",
    "对于映射$\\boldsymbol{f}:\\mathbb{R}^m \\to \\mathbb{R}^n$，其Jacobian矩阵$\\boldsymbol{J} \\in \\mathbb{R}^{n \\times m}$为\n",
    "\n",
    "> $J_{i,j} = \\frac{\\partial}{\\partial{x_j}}f(\\boldsymbol{x})_i$\n",
    "\n",
    "## Hessian矩阵\n",
    "Jacobian矩阵的梯度是Hessian矩阵$\\boldsymbol{H}(f)(\\boldsymbol{x})$， \n",
    "\n",
    "> $\\boldsymbol{H}(f)(\\boldsymbol{x})_{i,j} = \n",
    "\\frac{\\partial^2}{\\partial{x_i}\\partial{x_j}}f(\\boldsymbol{x})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
