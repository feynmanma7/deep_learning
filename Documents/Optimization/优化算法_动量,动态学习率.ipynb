{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optimization</h1>\n",
    "\n",
    "参考文献：\n",
    "[0] Deep Learning Book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 随机梯度下降Stachastic Gradient Descent\n",
    "\n",
    "## 0.0 SGD\n",
    "+ 对于<b>随机抽样</b>的$N$个mini-batch的样本，对参数$\\boldsymbol{\\theta}$计算梯度\n",
    "\n",
    "> $\\boldsymbol{g} = \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "L(f(\\boldsymbol{x}^{(i)}; \\boldsymbol \\theta), \\boldsymbol{y}^{(i)})$\n",
    "\n",
    "+ 更新参数\n",
    "\n",
    "> $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\epsilon \\boldsymbol{g}$\n",
    "\n",
    "## 0.1 动量角度理解\n",
    "\n",
    "+ 施加一定的力后速度变成, \n",
    "> $v \\leftarrow (- \\epsilon g)$\n",
    "\n",
    "+ 参数小球前进到位置, \n",
    "> $\\theta \\leftarrow \\theta + v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 动量 Momentum\n",
    "\n",
    "## 1.0 基于动量的优化过程\n",
    "\n",
    "物理学上动量定义, \n",
    "\n",
    "> $动量 = 质量 \\times 速度$\n",
    "\n",
    "在优化过程中，引入动量，为了让参数不偏离上一次运动的方向太多，从而加速收敛。\n",
    "\n",
    "基于动量的梯度下降模型假设: \n",
    "\n",
    "> + 优化过程定义为将一个小球（质量为1）, 从初始位置（如参数随机初始化）移动到最优（或者局部最优）位置\n",
    "\n",
    "> + 每一次移动小球的过程可以用一个梯度下降过程来模拟, 对小球施加一定的力量, 让小球瞬间（一个单位时间）达到一定的速度，按照这个速度运动一个单位, 到达相应单位之后速度可以选择归零（原始的梯度下降）或者保留（基于动量版本）\n",
    "\n",
    "动量守恒定律, \n",
    "\n",
    "> $F \\Delta t = m \\Delta v$\n",
    "\n",
    "移动过程中, 对小球施以力量$F$, 使得小球的运动速度变化为$\\Delta v$; \n",
    "\n",
    "对于凸问题, 可以让小球沿着梯度的负方向运动, 即对小球施以梯度负方向的力$ -g$, 乘以一个因子表示力的大小 $ \\epsilon (-g) = - \\epsilon g$; \n",
    "\n",
    "从而小球速度变化为,\n",
    "$\\Delta v = - \\epsilon g$\n",
    "\n",
    "## 1.0 Momentum计算过程\n",
    "\n",
    "+ 考虑保留随时间衰减的上一次的速度, 施加一定的力后, 速度变成, \n",
    "> $v \\leftarrow \\alpha v + (- \\epsilon g)$\n",
    "\n",
    "$\\alpha \\in [0, 1)$，衡量上次梯度指数衰减变化对当前影响大小. \n",
    "\n",
    "+ 参数移动到新的位置, \n",
    "> $\\theta \\leftarrow \\theta + v$\n",
    "\n",
    "\n",
    "## 1.1 Nesterov动量\n",
    "\n",
    "### (1) 更新参数\n",
    "> $ \\boldsymbol{ \\widetilde {\\theta} } = \\boldsymbol{\\theta} + \\alpha \\boldsymbol{v}$\n",
    "\n",
    "### (2) 计算梯度\n",
    "> $\\boldsymbol{g} = \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "L(f(\\boldsymbol{x}^{(i)}; \\boldsymbol { \\widetilde { \\theta } }), \\boldsymbol{y}^{(i)})$\n",
    "\n",
    "### (3) 计算动量\n",
    "\n",
    "与原始Momentum施加的力量的方向可能不同, Nesterov先<b>假设</b>按照上一步的速度（方向）移动小球, 再根据新的位置与最优位置的差异来计算移动方向, \n",
    "\n",
    "> $\\boldsymbol{v} = \\alpha \\boldsymbol{v} - \\epsilon \\boldsymbol{g}$\n",
    "\n",
    "### (4) 更新参数\n",
    "> $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} + \\epsilon \\boldsymbol{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自适应学习率 Adaptive Learning Rate\n",
    "\n",
    "## 2.1 AdaGrad\n",
    "\n",
    "> + 计算梯度: $g$\n",
    "> + 累积梯度平方: $r \\leftarrow r + g \\odot g$\n",
    "> + 速度变成: $v \\leftarrow \\frac{1}{\\sigma + \\sqrt{r}} \\odot ( - \\epsilon g)$\n",
    "> + 更新参数: $ \\theta + v $ ，$\\sigma$是一个很小的常数，如$1e{-7}$, \n",
    "\n",
    "累积梯度平方可能导致学习率下降过快。\n",
    "\n",
    "## 2.2 RMSProp\n",
    "\n",
    "采用梯度指数加权的移动平均来更新学习率，该方法经常被采用。\n",
    "\n",
    "RMSProp从来缓解AdaGrad学习率下降过快的缺点。\n",
    "\n",
    "> + 计算梯度: $g$\n",
    "> + 累积梯度指数加权的滑动平均: $r \\leftarrow \\rho r + (1 - \\rho) g \\odot g$\n",
    "> + 速度变成: $v \\leftarrow \\frac{1}{\\sqrt{\\sigma + r}} \\odot ( - \\epsilon g)$\n",
    "> + 更新参数: $\\theta \\leftarrow \\theta + v$ ，$\\sigma$是一个很小的常数，如$1e-7$\n",
    "\n",
    "## 2.3 Adam\n",
    "\n",
    "Adaptive Moments\n",
    "\n",
    "$\\rho_1, \\rho_2 \\in [0, 1)$，$\\rho_1$建议为0.9， $\\rho_2$建议为0.999\n",
    "\n",
    "> + $t \\gets t + 1$\n",
    "\n",
    "> 更新梯度的有偏一阶矩估计: $s \\leftarrow \\rho_1 s + (1 - \\rho_1) g $\n",
    "\n",
    "> 更新梯度的有偏二阶矩估计: $r \\leftarrow \\rho_2 r + (1 - \\rho_2) g \\odot g$\n",
    "\n",
    "> 修正一阶矩估计: $\\hat{s} \\leftarrow \\frac{s}{1 - \\rho_1}$\n",
    "\n",
    "> 修正二阶矩估计: $\\hat{r} \\leftarrow \\frac{r}{1 - \\rho_2}$\n",
    "\n",
    "> 速度更新为: $v \\leftarrow \\frac{\\hat{s}}{\\sigma + \\sqrt{\\hat{r}}} (- \\epsilon g)$\n",
    "\n",
    "> 参数更新为: $\\theta \\leftarrow \\theta + v$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
