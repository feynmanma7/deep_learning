{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "\n",
    "# 0. Xavier Initialization\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] Understanding the difficulty of training deep feedforward neural networks. Xaiver Glorot, Yoshua Bengio.\n",
    "\n",
    "## 0.0 Mean and Variance of Uniform distribution\n",
    "\n",
    "Assume $X \\sim U[a, b]$, \n",
    "\n",
    "> $X = \\int_{a}^b x dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mathbb{E}[x] = \\int_{a}^b \\frac{1}{b-a} x dx$\n",
    "\n",
    "> $= \\frac{1}{b-a} \\int_{a}^b x dx$\n",
    "\n",
    "> $= \\frac{1}{b-a} \\frac{1}{2} x^2 \\lvert_{a}^{b}$\n",
    "\n",
    "> $= \\frac{1}{b-a} \\frac{1}{2} (b^2 - a^2)$\n",
    "\n",
    "> $= \\frac{b+a}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mathbb{E}[x^2] = \\int_{a}^{b} \\frac{1}{b-a} x^2 dx$\n",
    "\n",
    "> $= \\frac{1}{b-a} \\frac{1}{3} x^3\\lvert_{a}^{b} $\n",
    "\n",
    "> $= \\frac{1}{b-a} \\frac{1}{3} (b^3 - a^3)$\n",
    "\n",
    "> $= \\frac{b^2 + ab + a^2}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Var[x] = \\mathbb{E}[x^2] - \\mathbb{E}^2[x]$\n",
    "\n",
    "> $= \\frac{b^2 + ab + a^2}{3} - (\\frac{a+b}{2})^2$\n",
    "\n",
    "> $= \\frac{(b-a)^2}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Uniform Initialization\n",
    "Initalize bias to be 0, and weight of each layer $W_{ij}$. \n",
    "\n",
    "Assume $W$ are drawn from unit distribution, $x_i$ and $w_i$ are dependent, \n",
    "\n",
    "> $s = \\sum_{i=1}^n w_i x_i$\n",
    "\n",
    "> $Var[s] = Var [\\sum_{i=1}^n w_i x_i]$\n",
    "\n",
    "> $= \\sum_{i=1}^n Var[w_i x_i]$\n",
    "\n",
    "> $= \\sum_{i=1}^n Var[w_i] Var[x_i]$\n",
    "\n",
    "If we want $Var[s] = Var[x_i]$, $\\sum_{i=1}^n Var[w_i] = 1$, $Var[w_i] = \\frac{1}{n}$.\n",
    "\n",
    "> $W_{ij} \\sim U[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}]$.\n",
    "\n",
    "\n",
    "> $Var[W] = \\frac{(\\frac{2}{\\sqrt{n}})^2}{12} = \\frac{1}{3n}$\n",
    "\n",
    "\n",
    "## 0.2 Xavier Intialization\n",
    "For a dense ANN using symmetric activation function $f$ with unit derivative at 0 \n",
    "($f^{'}(0)=1$).\n",
    "\n",
    "Let $s^{i}$ be the input of layer $i$, $z^{i}$ be the output of layer $i$,\n",
    "\n",
    "$W^{i}$ and $b^{i}$ are the weights and biases connect the output of layer $i-1$ and input of layer $i$, \n",
    "\n",
    "> $s^{i} = z^{i-1}W^{i} + b^{i}$\n",
    "\n",
    "> $z^{i} = f(s^{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $b^{i}=0$, \n",
    "\n",
    "> $z^{i} = f(z^{i-1}W^i + b^i)$\n",
    "\n",
    "> $= f(f(z^{i-2}W^{i-1}+b^{i-1})W^i + b^i)$\n",
    "\n",
    "> $= f(f...f(XW^1))$\n",
    "\n",
    "Assume that we are in the linear regime, \n",
    "\n",
    "> $f^{'}(s_{k}^{i}) \\approx 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Var[z^{i}] = Var[X]\\prod_{j=1}^{i-1}n^j Var[W^j]$\n",
    "\n",
    "$n^j$ is the size of layer $j-1$, where layer $0$ is the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a forward-propagation point of view, to keep information flowing, \n",
    "\n",
    "let the variance of the output in each layer be consistent,\n",
    "\n",
    "> $\\forall (i, j), Var[z^i] = Var[z^j]$\n",
    "\n",
    "> $\\Rightarrow$\n",
    "\n",
    "> + $n^{i}Var[W^i] = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a back-propagation view, to keep error flowing, \n",
    "\n",
    "let the variance of the gradient in each layer be consistent, \n",
    "\n",
    "> $\\frac{\\partial{Cost}}{\\partial{s^{i}}} = \n",
    "\\frac{\\partial{Cost}}{\\partial{s^{i+1}}} \n",
    "\\frac{\\partial{s^{i+1}}}{\\partial{z^{i}}} \n",
    "\\frac{\\partial{z^{i}}}{\\partial{s^{i}}} $\n",
    "\n",
    "> $= \\frac{\\partial{Cost}}{\\partial{s^{i+1}}} W^{i} f^{'}(s^i) $\n",
    "\n",
    "> $= \\frac{\\partial{Cost}}{\\partial{s^{i+1}}} W^{i}$\n",
    "\n",
    "> $= (\\frac{\\partial{Cost}}{\\partial{s^{i+2}}} W^{i+1}) W^{i}$\n",
    "\n",
    "> $= \\frac{\\partial{Cost}}{\\partial{s^{L}}} \\prod_{j=1}^{L-1}W^{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Var[\\frac{\\partial{Cost}}{\\partial{s_k^{i}}}] = \n",
    "Var[\\frac{\\partial{Cost}}{\\partial{s_k^{L}}}]\n",
    "\\prod_{j=i}^{L-1} n^{j+1} Var[W^j]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\forall(i, j), Var[\\frac{\\partial{Cost}}{\\partial{s_k^{i}}}] = Var[\\frac{\\partial{Cost}}{\\partial{s_k^{j}}}]$\n",
    "\n",
    "> $\\Rightarrow$\n",
    "\n",
    "> + $n^{i+1}Var[W^i] = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, \n",
    "\n",
    "> $n^{i}Var[W^i] = 1$\n",
    "\n",
    "> $n^{i+1}Var[W^i] = 1$\n",
    "\n",
    "let, \n",
    "\n",
    "> $Var[W^i] = \\frac{2}{n^i + n^{i+1}}$\n",
    "\n",
    "If $W \\sim U[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}]$, \n",
    "\n",
    "> $Var[W] = \\frac{1}{3n}$\n",
    "\n",
    "To obtain a uniform distribution of variance of weights, \n",
    "\n",
    "> $\\frac{2}{n^i + n^{i+1}} = \\frac{1}{3n}$\n",
    "\n",
    "> $\\Rightarrow$\n",
    "\n",
    "> $n = \\frac{n^i + n^{i+1}}{6}$\n",
    "\n",
    "+ Conclusion, \n",
    "Initialize each layer weight as, \n",
    "\n",
    "> $W^{i} \\sim U[- \\frac{1}{\\sqrt{ \\frac{n^i + n^{i+1}}{6} }}], \n",
    "\\frac{1}{\\sqrt{ \\frac{n^i + n^{i+1}}{6} }}]$\n",
    "\n",
    "> $W^{i} \\sim U[- \\frac{\\sqrt{6}}{\\sqrt{n^i + n^{i+1}}}, \n",
    "\\frac{\\sqrt{6}}{\\sqrt{n^i + n^{i+1}}}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. He Initialization\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] Delving Deep into Rectifiers- Surpassing Human-Level\n",
    "Performance on ImageNet Classification. Kaiming He, etc.\n",
    "\n",
    "Also assume that the elements in $x_l$ are also mutually independent \n",
    "and share the same distribution, and $x_l$ and $W_l$ are indpendent of each other.\n",
    "\n",
    "> $z^i = f(s^i)$\n",
    "\n",
    "> $s^i = z^{i-1}W^i + b^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Var[s^i] = n^i Var[z^{i-1} w^i + b^i]$\n",
    "\n",
    "Initialize $b_i$ to zero, based on the dependence,\n",
    "\n",
    "> $Var[s^i] = n^i Var[z^{i-1}]Var[w^i]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $Var[z^{i-1}] = Var[f(s^{i-1})]$\n",
    "\n",
    "If the activation function is ReLU, $f(x) = max(0, x)$, only half of x is activated, \n",
    "$Var[f] = \\frac{1}{2}Var[x]$,\n",
    "\n",
    "> $Var[s^i] = n^i (\\frac{1}{2} Var[s^{i-1}]) Var[w^i] \n",
    "= Var[s^{i-1}] (\\frac{1}{2} n^i Var[w^i])$\n",
    "\n",
    "> $\\Rightarrow$\n",
    "\n",
    "> $Var[s^L] = Var[s^1] \\prod_{i=2}^{L} (\\frac{1}{2} n^i Var[w^i])$\n",
    "\n",
    "To keep $Var[s^L] = Var[s^1]$, \n",
    "\n",
    "> $\\frac{1}{2} n^i Var[w^i] = 1$\n",
    "\n",
    "> $Var[w^i] = \\frac{2}{n^i}$\n",
    "\n",
    "If $w$ are drawn from Gaussian distribution with mean is zero, \n",
    "the variance for each layer is $\\frac{2}{n^i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
