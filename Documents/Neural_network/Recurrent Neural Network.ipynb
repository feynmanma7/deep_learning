{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Recurrent Neural Network</h1>\n",
    "\n",
    "# 0. Recurrent Neural Network\n",
    "\n",
    "考虑典型的循环神经网络，考虑输入长度为$\\tau$的时间序列。\n",
    "\n",
    "+ 节点\n",
    "\n",
    "> 输入节点：$x^{(t)} \\in \\mathbb{R}^{batch\\_size \\times n\\_input\\_unit}, t=1, 2, ..., \\tau$\n",
    "\n",
    "> 输出节点：$\\hat{y}^{(t)} \\in \\mathbb{R}^{batch\\_size \\times n\\_output\\_unit}$\n",
    "\n",
    "> 隐含节点：$h^{(t)} \\in \\mathbb{R}^{n\\_input\\_unit \\times n\\_output\\_unit}$\n",
    "\n",
    "+ 网络连接权重\n",
    "\n",
    "> 输入到隐含节点：$U \\in \\mathbb{R}^{n\\_input\\_unit \\times n\\_hidden\\_unit}$\n",
    "\n",
    "> 隐含到输出节点：$V \\in \\mathbb{R}^{n\\_hidden\\_unit \\times n\\_output\\_unit}$\n",
    "\n",
    "> 隐含到隐含节点: $W \\in \\mathbb{R}^{n\\_hidden\\_unit \\times n\\_hidden\\_unit}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forward Propagation\n",
    "\n",
    "对于$t \\in \\{1, 2, ..., \\tau\\}$，依次计算$h^{(t)}$及$\\hat{y}^{(t)}$，\n",
    "\n",
    "+ 节点计算规则\n",
    "\n",
    "> 隐含节点输入: $a^{(t)} = Wh^{(t-1)} + Ux^{(t)} + b$\n",
    "\n",
    "> 隐含节点输出: $h^{(t)} = tanh(a^{(t)})$\n",
    "\n",
    "> 输入节点输入: $o^{(t)} = Vh^{(t)} + c$\n",
    "\n",
    "> 输出节点输出: $\\hat y^{(t)} = softmax(o^{(t)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Back Propagation Error\n",
    "\n",
    "## 2.0 损失函数\n",
    "交叉熵cross-entropy。考虑包含$N$个样本的mini-batch平均误差，每个样本的误差是所有$\\tau$个时间序列的误差之和；\n",
    "\n",
    "> $L_{mini\\_batch} = \\frac{1}{N}\\sum_{n=1}^{N}L_n$\n",
    "\n",
    "> $L_n = \\sum_{t=1}^{\\tau} L_n^{(t)}$\n",
    "\n",
    "> $L_n^{(t)} = - y_n^{(t)} \\log \\hat{y}_n^{(t)}$\n",
    "\n",
    "## 2.1 对权重$U,V,W$的偏导\n",
    "对于$t \\in \\{\\tau, \\tau - 1, ..., 2, 1\\}$，依次计算mini_batch的损失$L_{mini\\_batch}$对各个权重$U$、$V$以及$W$的偏导，\n",
    "\n",
    "> $\\frac{\\partial{L}}{\\partial{U_{ij}}} \n",
    "= \\frac{1}{N} \\sum_{n=1}^{N}\\frac{\\partial{L_n}}{\\partial{U_{ij}}}$\n",
    "\n",
    "所以，只需要计算每个样本$L_n$对$U$、$V$以及$W$的偏导，然后求均值即可。为书写清洁，将$L_n$简写为$L$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里偏导、梯度、转置的关系有待进一步理解。\n",
    "\n",
    "> $\\nabla_{c}{L} = \\sum_{t}\\left( \n",
    "\\frac{\\partial{o^{(t)}}}{\\partial{c}} \\right) ^ {T}\n",
    "\\nabla_{o^{(t)}}{L}$\n",
    "\n",
    "> $\\nabla_{b}{L} = \\sum_{t}\\left( \n",
    "\\frac{\\partial{h^{(t)}}}{\\partial{b}} \\right) ^ {T} \\nabla_{h^{(t)}}{L}\n",
    "= \\sum_t diag \\left( 1 - (h^{(t)})^2 \\right) \\nabla_{h^{(t)}}{L}$ \n",
    "\n",
    "> $\\nabla_{V}{L} = \\sum_{t} \\sum_{i}\n",
    "\\left(  \\frac{\\partial{L}}{\\partial_{o_i^{(t)}}} \\right) \\nabla_{V}{o_i^{(t)}} \n",
    "= \\sum_t \\left( \\nabla_{V}{o^{(t)}}\\right) {h^{(t)}}^{T} $\n",
    "\n",
    "> $\\nabla_{W}{L} = \\sum_t \\sum_i\n",
    "\\left( \\frac{\\partial{L}}{\\partial{h_i^{(t)}}} \\right) \\nabla_{W}{h_i^{(t)}}\n",
    "=\\sum_t diag \\left( 1 - (h^{(t)})^2 \\right) (\\nabla_{h^{t}}{L}) {h^{(t-1)}}^T$\n",
    "\n",
    "> $\\nabla_{U}{L} = \\sum_t \\sum_i\n",
    "\\left( \\frac{\\partial_{L}}{\\partial{h_i^{(t)}}} \\right) \\nabla_{U}{h_i^{(t)}}\n",
    "= \\sum_t diag \\left( 1 - (h^{(t)})^2 \\right) (\\nabla_{h^{t}}{L}) {x^{(t)}}^T$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1.0 计算并保存$\\nabla_{o^{(t)}}L$及$\\nabla_{h^{(t)}}L$\n",
    "\n",
    "softmax求导;\n",
    "> $f(x_i) = \\frac{e^{x_i}}{\\sum_{j}e^{x_j}}$\n",
    "\n",
    "> $\\frac{\\partial{f(x_i)}}{\\partial{x_i}} = \n",
    "e^{x_i} * \\left(\\frac{1}{\\sum_{j}e^{x_j}}\\right) + \n",
    "e^{x_i} * \\left(- \\frac{1}{\\left(\\sum_{j}e^{x_j}\\right)^2} * e^{x_i}\\right)$\n",
    "\n",
    "> $ = \\frac{e^{x_i}}{\\sum_{j}e^{x_j}} \\left( 1 - \\frac{e^{x_i}}{\\sum_{j}e^{x_j}} \\right)$\n",
    "\n",
    "所以softmax的导数与sigmoid一样的形式，\n",
    "> $f^{'}(x_i) = f(x_i) \\left(1 - f(x_i)\\right)$\n",
    "\n",
    "#### 2.1.0.0 计算$\\nabla_{o^{(t)}}L$\n",
    "\n",
    "> $\\left(\\nabla_{o^{(t)}}L\\right)_i = \\frac{\\partial{L}}{\\partial{L^{(t)}}}\n",
    "\\frac{\\partial{L_t}}{\\partial{o_i^{(t)}}}$\n",
    "\n",
    "> $ = 1 * \\frac{\\partial{L_t}} {\\partial{\\hat{y_i}^{(t)}}}\n",
    "\\frac{\\partial{\\hat{y_i}^{(t)}}} {\\partial{o_i^{(t)}}} $\n",
    "\n",
    "> $ = \\left( -\\frac{y_i^{(t)}}{\\hat{y_i}^{(t)}} \\right)\n",
    "\\left( \\hat{y_i}^{(t)} (1 - \\hat{y_i}^{(t)})\\right)$\n",
    "\n",
    "> $ = (\\hat{y_i}^{(t)} - 1) y_i^{(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\nabla_{o^{(t)}}L = (\\hat{y}^{(t)} - 1) y^{(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.0.1 计算$\\nabla_{h^{(t)}}L$\n",
    "\n",
    "+ 最后一个时间步，$h^{(\\tau)}$的误差只受$o^{(\\tau)}$的误差影响；\n",
    "\n",
    "> $\\left(\\nabla_{h^{(\\tau)}}L \\right)_i = \n",
    "\\frac{\\partial{L}}{\\partial{o_i^{(\\tau)}}}$\n",
    "\n",
    "> $\\nabla_{h^{(\\tau)}}L = \\frac{\\partial{o_i^{(\\tau)}}}{\\partial{h_i^{(\\tau)}}}\n",
    "= V^{T} \\nabla_{o^{(\\tau)}}L$\n",
    "\n",
    "+ 时间步$t = 0,1,...,\\tau-1$，$h^{(t)}$的误差受$o^{(t)}$的误差以及$h^{(t+1)}$的误差影响；\n",
    "\n",
    "> $\\left(\\nabla_{h^{(t)}}L \\right)_i = \n",
    "\\frac{\\partial{L}}{\\partial{o_i^{(t)}}}\n",
    "\\frac{\\partial{o_i^{(t)}}}{\\partial{h_i^{(t)}}} + \n",
    "\\frac{\\partial{L}}{\\partial{h_i^{(t+1)}}}\n",
    "\\frac{\\partial{h_i^{(t+1)}}}{\\partial{h_i^{(t)}}}$\n",
    "\n",
    "> $\\nabla_{h^{(t)}}L = \n",
    "V^{T} \\nabla_{o^{(t)}}L +\n",
    "W^{T} \\left(\\nabla_{h^{(t+1)}}L\\right) diag \\left( 1 - h^{(t+1)} \\right)^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Update Weights\n",
    "\n",
    "对于mini-batch，\n",
    "\n",
    "> $U_{ij} = U_{ij} - \\eta_u * \\frac{\\partial{L}}{\\partial{U_{ij}}}$\n",
    "\n",
    "> $V_{ij} = V_{ij} - \\eta_v * \\frac{\\partial{L}}{\\partial{V_{ij}}}$\n",
    "\n",
    "> $W_{ij} = W_{ij} - \\eta_w * \\frac{\\partial{L}}{\\partial{W_{ij}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
