{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Representation Learning</h1>\n",
    "\n",
    "# 1. 主成分分析PCA\n",
    "\n",
    "## 1.1 最大方差法\n",
    "\n",
    "以1-D数据为例，对于$N$条样本数据$\\boldsymbol{X}\\in \\mathbb{R}$，假设投影到单位向量$\\boldsymbol{u}$所在的方向上。\n",
    "\n",
    "$\\boldsymbol{X}$的方差为\n",
    "\n",
    "> $\\boldsymbol{S} = \\sum_{n=1}^{N}(\\boldsymbol{x}^{(n)} - \\boldsymbol{\\bar{x}})^2$\n",
    "\n",
    "样本$\\boldsymbol{x}^{(n)}$投影后的向量为$\\boldsymbol{u}^{\\top} \\boldsymbol{x}^{(n)}$。\n",
    "\n",
    "$\\boldsymbol{X}$投影后的方差为\n",
    "\n",
    "> $\\sum_{n=1}^{N}(\\boldsymbol{u}^{\\top}\\boldsymbol{x}^{(n)} - \\boldsymbol{u}^{\\top}\\boldsymbol{\\bar{x}})^2$\n",
    "\n",
    "> $= \\boldsymbol{u}^{\\top} \n",
    "\\sum_{n=1}^{N}(\\boldsymbol{x}^{(n)} - \\boldsymbol{\\bar{x}})^2\n",
    "\\boldsymbol{u}$\n",
    "\n",
    "> $= \\boldsymbol{u}^{\\top} \\boldsymbol{S} \\boldsymbol{u}$\n",
    "\n",
    "为了最大限度保留数据$\\boldsymbol{S}$的信息，需要让投影后的方差最大。\n",
    "考虑约束$\\boldsymbol{u}^{\\top}\\boldsymbol{u} = 1$，引入Lagrange乘子$\\lambda$，\n",
    "\n",
    "> $\\max{(-J)} = \\boldsymbol{u}^{\\top} \\boldsymbol{S} \\boldsymbol{u}$\n",
    "> $+ \\lambda(1 - \\boldsymbol{u}^{\\top} \\boldsymbol{u} )$\n",
    "\n",
    "计算\n",
    "\n",
    "> $\\frac{ \\partial{(-J)} }\n",
    "{ \\partial{\\boldsymbol{u}} }$\n",
    "> $= (\\boldsymbol{S} + {\\boldsymbol{S}}^{\\top})\\boldsymbol{u} - \n",
    "2\\lambda \\boldsymbol{u} =\n",
    "2 (\\boldsymbol{S} \\boldsymbol{u} - \\lambda \\boldsymbol{u} )\n",
    " = 0$\n",
    "\n",
    "从而， \n",
    "> $\\boldsymbol{S}\\boldsymbol{u} = \\lambda \\boldsymbol{u}$\n",
    "\n",
    "即为了让投影后方差最大，应投影$\\boldsymbol{X}$到其协方差矩阵的最大的特征值所在的特征向量上。$\\lambda$是$\\boldsymbol{S}$最大的特征值，$\\boldsymbol{u}$是$\\boldsymbol{S}$最大的特征值所在的特征向量。\n",
    "\n",
    "对$M$维数据用数学归纳法推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 最小误差法\n",
    "\n",
    "假设$M$维数据$\\boldsymbol{X}$投影到$D<M$维上。\n",
    "\n",
    "> $ \\tilde{ \\boldsymbol{x}} ^ {(n)} = \n",
    "\\sum_{i=1}^{D}\n",
    "(\\boldsymbol{z}_{ni})\n",
    "\\boldsymbol{u}_i + \n",
    "\\sum_{i=D+1}^{M} \\boldsymbol{b}_i^{\\top} \\boldsymbol{u}_i\n",
    "$\n",
    "\n",
    "要让投影总损失最小，考虑约束$\\boldsymbol{u}^{\\top}\\boldsymbol{u} = 1$，引入Lagrange乘子$\\lambda$。\n",
    "\n",
    "> $\\min J = \\frac{1}{N}\\sum_{n=1}^{N} ||\\boldsymbol{x} ^ {(n)} - \\tilde{ \\boldsymbol{x}} ^ {(n)}||^2 \n",
    "+ \\lambda(1 - \\boldsymbol{u}^{\\top}\\boldsymbol{u})$\n",
    "\n",
    "计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + $\\frac{\\partial{J}}\n",
    "{\\partial{\\boldsymbol{z}_{nj}}} = \n",
    "\\frac{2}{N}(\\tilde{ \\boldsymbol{x}} ^ {(n)} - \\boldsymbol{x} ^ {(n)}) ^{\\top} \\boldsymbol{u}_j$\n",
    "\n",
    "> $ = \\frac{2}{N}\n",
    "(\\boldsymbol{u}_j^{\\top} \\boldsymbol{z}_{nj} \\boldsymbol{u}_j - \n",
    "{\\boldsymbol{x} ^ {(n)}}^{\\top}\\boldsymbol{u}_j\n",
    ")$\n",
    "\n",
    "> $ = \\frac{2}{N}\n",
    "(\\boldsymbol{z}_{nj} - {\\boldsymbol{x} ^ {(n)}}^{\\top}\\boldsymbol{u}_j\n",
    ") = 0\n",
    "$\n",
    "\n",
    "> $ \\Rightarrow \\boldsymbol{z}_{nj} = {\\boldsymbol{x} ^ {(n)}}^{\\top}\\boldsymbol{u}_j\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + $\\frac{\\partial{J}}{\\partial{b_j}} = \n",
    "\\frac{2}{N} \n",
    "\\sum_{n=1}^{N} \n",
    "(\\tilde{ \\boldsymbol{x}} ^ {(n)} - \\boldsymbol{x} ^ {(n)}) ^{\\top}\n",
    "\\boldsymbol{u}_j\n",
    "$\n",
    "\n",
    "> $ = \\frac{2}{N} \n",
    "\\sum_{n=1}^{N} (\n",
    "\\boldsymbol{u}_j^{\\top} \\boldsymbol{b}_j \\boldsymbol{u}_j - \n",
    "{\\boldsymbol{x} ^ {(n)}} ^{\\top} \\boldsymbol{u}_j) = 0\n",
    "$\n",
    "\n",
    "> $\\Rightarrow  \\boldsymbol{b}_j = \n",
    "\\frac{1}{N}\\sum_{n=1}^{N} \\boldsymbol{x} ^ {(n)}) ^{\\top}\n",
    "\\boldsymbol{u}_j = \n",
    "\\bar{\\boldsymbol{x}}^{\\top} \\boldsymbol{u}_j\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + $\\boldsymbol{x} ^ {(n)} - \\tilde{ \\boldsymbol{x}} ^ {(n)}$\n",
    "\n",
    "> $ = \\sum_{i=1}^{M} {\\boldsymbol{x} ^ {(n)}} ^{\\top}  \\boldsymbol{u}_i  - \n",
    "(\\sum_{i=1}^{D} {\\boldsymbol{x} ^ {(n)}} ^ {\\top} \\boldsymbol{u}_i + \n",
    " \\sum_{i=D+1}^{M} {\\bar{\\boldsymbol{x}}} ^ {\\top} \\boldsymbol{u}_i\n",
    ")$\n",
    "\n",
    "> $ = \\sum_{i=D+1}^{M} (\n",
    "{\\boldsymbol{x} ^ {(n)}} ^{\\top}  \\boldsymbol{u}_i - {\\bar{\\boldsymbol{x}}} ^ {\\top} \\boldsymbol{u}_i\n",
    ") $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + $J = \\frac{1}{N}\\sum_{n=1}^{N}\n",
    "\\sum_{i=D+1}^{M} (\n",
    "{\\boldsymbol{x} ^ {(n)}} ^{\\top}  \\boldsymbol{u}_i - {\\bar{\\boldsymbol{x}}} ^ {\\top} \\boldsymbol{u}_i\n",
    ") ^ 2 +\n",
    "\\lambda( \\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1)\n",
    "$\n",
    "\n",
    "> $ = \\sum_{i=D+1}^{M} {\\boldsymbol{u}_i}^{\\top} \\boldsymbol{S} \\boldsymbol{u}_i +\n",
    "\\lambda(1 - \\boldsymbol{u}^{\\top}\\boldsymbol{u})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + $\\frac{\\partial{J}}{\\partial{\\boldsymbol{u}_j}}  \n",
    "= (\\boldsymbol{S} + {\\boldsymbol{S}}^{\\top} )\\boldsymbol{u}_j - 2\\lambda \\boldsymbol{u}_j = 0\n",
    "$\n",
    "\n",
    "> $\\Rightarrow \\boldsymbol{S} \\boldsymbol{u}_j = \\lambda \\boldsymbol{u}_j$\n",
    "\n",
    "为了让$J$最小，$\\boldsymbol{u}_j$是$\\boldsymbol{S}$的最小特征值所在的特征向量，$\\lambda$是$\\boldsymbol{S}$的最小特征值。\n",
    "\n",
    "因此，$\\boldsymbol{X}$应该被投影到其协方差矩阵特征值前$D$大个特征值所在的特征向量上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 最小重建误差法\n",
    "\n",
    "给定输入$\\boldsymbol{x} \\in \\mathbb{R}^{M}$，选取编码函数$f$及解码函数$g$。将输入编码为$\\boldsymbol{c} \\in \\mathbb{R}^{L}$，解码重建为$r(\\boldsymbol{x}) \\in \\mathbb{R}^{M}$，需要最小化重建误差。\n",
    "\n",
    "> $\\boldsymbol{c}^{*}=\\arg\\min_{\\boldsymbol{c}}||\\boldsymbol{x}-g(c)||_2$\n",
    "\n",
    "假设通过线性变换来进行编码与解码，从而可定义解码变换矩阵为$\\boldsymbol{D} \\in \\mathbb{R}^{M \\times L}$。不是一般性（为计算方便），可假设$\\boldsymbol{D}$是$L$维单位正交矩阵。\n",
    "\n",
    "> $g(\\boldsymbol{c}) = \\boldsymbol{D}\\boldsymbol{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\boldsymbol{c}^{*}=\\arg\\min_{\\boldsymbol{c}}||\n",
    "\\boldsymbol{x} -\n",
    "\\boldsymbol{D}\\boldsymbol{c}\n",
    "||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $||\n",
    "\\boldsymbol{x} -\n",
    "\\boldsymbol{D}\\boldsymbol{c}\n",
    "||_2 = \n",
    "(\\boldsymbol{x} -\n",
    "\\boldsymbol{D}\\boldsymbol{c})^{\\top}\n",
    "(\\boldsymbol{x} -\n",
    "\\boldsymbol{D}\\boldsymbol{c})\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $ = (\\boldsymbol{x}^{\\top} - (\\boldsymbol{Dc})^{\\top})\n",
    "(\\boldsymbol{x} - \\boldsymbol{D}\\boldsymbol{c})$\n",
    "\n",
    "> $ = {\\boldsymbol{x}}^{\\top}\\boldsymbol{x}\n",
    "- 2{\\boldsymbol{x}}^{\\top}(\\boldsymbol{Dc})\n",
    "+ {\\boldsymbol{c}}^{\\top}{\\boldsymbol{D}}^{\\top}\\boldsymbol{D}\\boldsymbol{c} $\n",
    "\n",
    "其中，$({\\boldsymbol{Dc}})^{\\top}\\boldsymbol{x} \n",
    "= {\\boldsymbol{x}}^{\\top}(\\boldsymbol{Dc})$是常数。\n",
    "第一项${\\boldsymbol{x}}^{\\top}\\boldsymbol{x}$与$\\boldsymbol{c}$无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\boldsymbol{c}^{*}=\\arg\\min_{\\boldsymbol{c}}\n",
    "- 2{\\boldsymbol{x}}^{\\top}(\\boldsymbol{Dc})\n",
    "+ {\\boldsymbol{c}}^{\\top}{\\boldsymbol{D}}^{\\top}\\boldsymbol{D}\\boldsymbol{c}$\n",
    "\n",
    "> $ = - 2{\\boldsymbol{x}}^{\\top}(\\boldsymbol{Dc})\n",
    "+ {\\boldsymbol{c}}^{\\top}\\boldsymbol{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{c}^{*}$对$\\boldsymbol{c}$求偏导，并令其等于0，\n",
    "\n",
    "> $\\nabla_{\\boldsymbol{c}}\\boldsymbol{c}^{*} = \n",
    "-2 {(\\boldsymbol{x}^{\\top}\\boldsymbol{D})}^{\\top} + \n",
    "2 \\boldsymbol{c} = 0$\n",
    "\n",
    "> $\\Rightarrow \\boldsymbol{c} = {\\boldsymbol{D}}^{\\top}\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入优化目标，得到关于$\\boldsymbol{D}$的新的优化目标。\n",
    "对所有样本使用相同的编码矩阵${\\boldsymbol{D}}^{\\top}$。\n",
    "\n",
    "优化目标变成对所有样本的重建误差矩阵\n",
    "$\\boldsymbol{E} = \\boldsymbol{X} - \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X} $\n",
    "的Forbenious范数的优化。\n",
    "\n",
    "> $\\boldsymbol{D}^{*} = \\arg\\min_{\\boldsymbol{D}} \n",
    "||E||_F$\n",
    "\n",
    "> $s.t. {\\boldsymbol{D}}^{\\top}\\boldsymbol{D} = \\boldsymbol{I}_l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\boldsymbol{D}^{*} = \\arg\\min_{\\boldsymbol{D}} \n",
    "\\sqrt{ tr( \\boldsymbol{E}{\\boldsymbol{E}}^{\\top} ) }$ \n",
    "\n",
    "> $ = \\arg\\min_{\\boldsymbol{D}}\n",
    "tr(\\boldsymbol{E}{\\boldsymbol{E}}^{\\top}) $\n",
    "\n",
    "> $ = \\arg\\min_{\\boldsymbol{D}}\n",
    "tr({(\\boldsymbol{X} - \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X})}^{\\top}\n",
    "(\\boldsymbol{X} - \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}) )\n",
    "$\n",
    "\n",
    "> $ = \\arg\\min_{\\boldsymbol{D}}\n",
    "tr(\n",
    "({\\boldsymbol{X}}^{\\top} - {\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} )\n",
    "(\\boldsymbol{X} - \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X})\n",
    ")\n",
    "$\n",
    "\n",
    "> $ = \\arg\\min_{\\boldsymbol{D}}\n",
    "tr(\n",
    "{\\boldsymbol{X}}^{\\top}\\boldsymbol{X}\n",
    "- {\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    "- {\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    "+ {\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top}\n",
    "\\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    ")\n",
    "$\n",
    "\n",
    "> $ = \\arg\\min_{\\boldsymbol{D}}\n",
    "tr(\n",
    "- 2{\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    "+ {\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top}\n",
    "\\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    ")\n",
    "$\n",
    "\n",
    "$tr({\\boldsymbol{X}}^{\\top}\\boldsymbol{X})$与$\\boldsymbol{D}$无关。\n",
    "根据约束${\\boldsymbol{D}}^{\\top} \\boldsymbol{D} = \\boldsymbol{I}_l$，且trace具有循环性，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\boldsymbol{D}^{*} = \\arg\\min_{\\boldsymbol{D}}\n",
    "tr(\n",
    "- 2{\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    "+ {\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    ")\n",
    "$\n",
    "\n",
    "> $ =  \\arg\\min_{\\boldsymbol{D}}\n",
    "tr(-\n",
    "{\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    ")\n",
    "$\n",
    "\n",
    "> $ = \\arg\\max_{\\boldsymbol{D}}\n",
    "tr(\n",
    "{\\boldsymbol{X}}^{\\top} \\boldsymbol{D} {\\boldsymbol{D}}^{\\top} \\boldsymbol{X}\n",
    ") $\n",
    "\n",
    "> $ = \\arg\\max_{\\boldsymbol{D}}\n",
    "tr(\n",
    "{\\boldsymbol{D}}^{\\top} \\boldsymbol{X} {\\boldsymbol{X}}^{\\top} \\boldsymbol{D}\n",
    ")\n",
    "$\n",
    "\n",
    "当$\\boldsymbol{D}$由$\\boldsymbol{X} {\\boldsymbol{X}}^{\\top}$最大的$L$个特征值所在的特征向量构成，能获得最小的重建误差及最优编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自动编码机AutoEncoder\n",
    "\n",
    "> 目标函数：$\\min L(\\boldsymbol{x}, g(f(\\boldsymbol{x})))$，如果$L$是平方损失，AE等价于PCA。\n",
    "\n",
    "PCA的$f$与$g$是线性变换（矩阵变换），如果使用非线性的$f$与$g$，则是普通意义上的AE，具有更强大的自编码能力。\n",
    "\n",
    "但是，如果不对AE加以一定的限制，给予无限的学习能力，AE可能会过拟合，学习不到有意义的隐藏特征。例如，AE编码到一维，标识该样本在数据集中所在维度（深度过拟合）。\n",
    "\n",
    "## 2.1 正则化自动编码机 Regularized AutoEncoder\n",
    "\n",
    "### 2.1.1 稀疏自动机\n",
    "\n",
    "对编码层加上得到稀疏控制的惩罚，\n",
    "\n",
    "> 重建误差: $\\min L(\\boldsymbol{x}, g(f(\\boldsymbol{x}))) + \\Omega(\\boldsymbol{h})$，典型地，$\\boldsymbol{h}=f(\\boldsymbol{x})$\n",
    "\n",
    "一般可选择：\n",
    "\n",
    "> + Laplace先验：$p_{model}(\\boldsymbol{h}_i) = \n",
    "\\frac{\\lambda}{2}e^{-\\lambda |\\boldsymbol{h}_i|}$\n",
    "\n",
    "> + Student-t先验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
