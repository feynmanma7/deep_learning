{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optimization</h1>\n",
    "\n",
    "参考文献：\n",
    "[0] Deep Learning Book.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Optimization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 随机梯度下降Stachastic Gradient Descent\n",
    "\n",
    "## (1) 对于$N$个mini-batch的样本，对参数$\\boldsymbol{\\theta}$计算梯度\n",
    "\n",
    "> $\\boldsymbol{g} = \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "L(f(\\boldsymbol{x}^{(i)}; \\boldsymbol \\theta), \\boldsymbol{y}^{(i)})$\n",
    "\n",
    "## (2) 更新参数\n",
    "\n",
    "> $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\epsilon \\boldsymbol{g}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 动量 Momentum\n",
    "\n",
    "物理学上\n",
    "\n",
    "> $动量 = 质量 \\times 速度$\n",
    "\n",
    "在优化过程中，引入动量，为了让参数不偏离上一次运动的方向太多，从而加速收敛。\n",
    "\n",
    "### （1）计算动量\n",
    "\n",
    "> $\\boldsymbol{v} = \\alpha \\boldsymbol{v} - \\epsilon \\boldsymbol{g}$\n",
    "\n",
    "$\\alpha \\in [0, 1)$，衡量上次梯度指数衰减变化对当前影响大小。\n",
    "\n",
    "### （2）更新参数\n",
    "\n",
    "> $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} + \\epsilon \\boldsymbol{v}$\n",
    "\n",
    "\n",
    "## 1.1 Nesterov动量\n",
    "\n",
    "### (1) 更新参数\n",
    "> $ \\boldsymbol{ \\widetilde {\\theta} } = \\boldsymbol{\\theta} + \\alpha \\boldsymbol{v}$\n",
    "\n",
    "### (2) 计算梯度\n",
    "> $\\boldsymbol{g} = \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "L(f(\\boldsymbol{x}^{(i)}; \\boldsymbol { \\widetilde { \\theta } }), \\boldsymbol{y}^{(i)})$\n",
    "\n",
    "### (3) 计算动量\n",
    "\n",
    "> $\\boldsymbol{v} = \\alpha \\boldsymbol{v} - \\epsilon \\boldsymbol{g}$\n",
    "\n",
    "### (4) 更新参数\n",
    "> $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} + \\epsilon \\boldsymbol{v}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自适应学习率 Adaptive Learning Rate\n",
    "\n",
    "## 2.1 AdaGrad\n",
    "\n",
    "> + 计算梯度： $\\boldsymbol{g}$\n",
    "> + 累积梯度平方：$\\boldsymbol{r} \\gets \\boldsymbol{r} + \\boldsymbol{g} \\odot \\boldsymbol{g}$\n",
    "> + 更新参数：$\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \n",
    "\\frac{\\epsilon}{\\sigma + \\sqrt{\\boldsymbol{r}}} \\odot \\boldsymbol{g}$ ，$\\sigma$是一个很小的常数，如${10}^{-7}$\n",
    "\n",
    "累积梯度平方可能导致学习率下降过快。\n",
    "\n",
    "## 2.2 RMSProp\n",
    "\n",
    "采用梯度指数加权的移动平均来更新学习率，该方法经常被采用。\n",
    "\n",
    "> + 计算梯度：$\\boldsymbol{g}$\n",
    "> + 累积梯度指数加权的滑动平均：$\\boldsymbol{r} \\gets \\rho \\boldsymbol{r} + (1 - \\rho) \\boldsymbol{g} \\odot \\boldsymbol{g} $\n",
    "> + 更新参数：$\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \n",
    "\\frac{\\epsilon}{\\sigma + \\sqrt{\\boldsymbol{r}}} \\odot \\boldsymbol{g}$ ，$\\sigma$是一个很小的常数，如${10}^{-7}$\n",
    "\n",
    "## 2.3 Adam\n",
    "\n",
    "Adaptive Moments\n",
    "\n",
    "$\\rho_1, \\rho_2 \\in [0, 1)$，$\\rho_1$建议为0.9， $\\rho_2$建议为0.999\n",
    "\n",
    "> + $t \\gets t + 1$\n",
    "> + 更新有偏一阶矩：$\\boldsymbol{s} \\gets \\rho_1 \\boldsymbol{s} + (1 - \\rho_1) \\boldsymbol{g}$\n",
    "> + 更新有偏二阶矩：$\\boldsymbol{r} \\gets \\rho_2 \\boldsymbol{r} + (1 - \\rho_2) \\boldsymbol{g} \\odot \\boldsymbol{g}$\n",
    "> + 修正一阶矩偏差：$\\boldsymbol{\\hat{s}} \\gets \\frac{\\boldsymbol{s}}{1 - \\rho_1^t}$\n",
    "> + 修正二阶矩偏差：$\\boldsymbol{\\hat{r}} \\gets \\frac{\\boldsymbol{r}}{1 - \\rho_2^t}$\n",
    "> + 更新参数：$\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \n",
    "\\epsilon \n",
    "\\frac{\\boldsymbol{\\hat{s}}}{\\sigma + \\sqrt{\\boldsymbol{\\hat{r}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 二阶近似方法 Second Order Methods\n",
    "\n",
    "## 3.1 牛顿法\n",
    "\n",
    "由泰勒展开，\n",
    "\n",
    "> $f(x+\\Delta{x}) \\approx f(x) + \\Delta{x}^{\\top}g + \\frac{1}{2}\\Delta{x}^{\\top}H\\Delta{x}$\n",
    "\n",
    "> $y = f(x+\\Delta{x}) - f(x) \\approx \\Delta{x}^{\\top}g + \\frac{1}{2}\\Delta{x}^{\\top}H\\Delta{x}$\n",
    "\n",
    "> 令，$\\nabla_{\\Delta{x}}y = g + \\frac{1}{2}(H+H^{\\top})\\Delta{x} = g + H\\Delta{x} = 0$，得到\n",
    "\n",
    "> $\\Delta{x} = - H ^{-1}g$\n",
    "\n",
    "所以更新参数规则为，\n",
    "\n",
    "> $x \\gets x - \\epsilon ^ {-1} g$\n",
    "\n",
    "每次迭代需要计算Hessian矩阵（$\\mathbb{R}^{k \\times k}$）的逆，时间复杂度为$O(k^3)$，计算量太大。\n",
    "\n",
    "## 3.2 共轭梯度法\n",
    "\n",
    "## 3.3 BFGS\n",
    "\n",
    "找到$H^{-1}$的近似矩阵。\n",
    "\n",
    "## 3.4 L-BFGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 批标准化 Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
