{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optimization</h1>\n",
    "\n",
    "参考文献：\n",
    "[0] Deep Learning Book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 随机梯度下降Stachastic Gradient Descent\n",
    "\n",
    "## 0.0 SGD\n",
    "+ 对于<b>随机抽样</b>的$N$个mini-batch的样本，对参数$\\boldsymbol{\\theta}$计算梯度\n",
    "\n",
    "> $\\boldsymbol{g} = \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "L(f(\\boldsymbol{x}^{(i)}; \\boldsymbol \\theta), \\boldsymbol{y}^{(i)})$\n",
    "\n",
    "+ 更新参数\n",
    "\n",
    "> $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\epsilon \\boldsymbol{g}$\n",
    "\n",
    "## 0.1 动量角度理解\n",
    "\n",
    "+ 施加一定的力后速度变成, \n",
    "> $v \\leftarrow (- \\epsilon g)$\n",
    "\n",
    "+ 参数小球前进到位置, \n",
    "> $\\theta \\leftarrow \\theta + v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 动量 Momentum\n",
    "\n",
    "## 1.0 基于动量的优化过程\n",
    "\n",
    "物理学上动量定义, \n",
    "\n",
    "> $动量 = 质量 \\times 速度$\n",
    "\n",
    "在优化过程中，引入动量，为了让参数不偏离上一次运动的方向太多，从而加速收敛。\n",
    "\n",
    "基于动量的梯度下降模型假设: \n",
    "\n",
    "> + 优化过程定义为将一个小球（质量为1）, 从初始位置（如参数随机初始化）移动到最优（或者局部最优）位置\n",
    "\n",
    "> + 每一次移动小球的过程可以用一个梯度下降过程来模拟, 对小球施加一定的力量, 让小球瞬间（一个单位时间）达到一定的速度，按照这个速度运动一个单位, 到达相应单位之后速度可以选择归零（原始的梯度下降）或者保留（基于动量版本）\n",
    "\n",
    "动量守恒定律, \n",
    "\n",
    "> $F \\Delta t = m \\Delta v$\n",
    "\n",
    "移动过程中, 对小球施以力量$F$, 使得小球的运动速度变化为$\\Delta v$; \n",
    "\n",
    "对于凸问题, 可以让小球沿着梯度的负方向运动, 即对小球施以梯度负方向的力$ -g$, 乘以一个因子表示力的大小 $ \\epsilon (-g) = - \\epsilon g$; \n",
    "\n",
    "从而小球速度变化为,\n",
    "$\\Delta v = - \\epsilon g$\n",
    "\n",
    "## 1.0 Momentum计算过程\n",
    "\n",
    "+ 考虑保留随时间衰减的上一次的速度, 施加一定的力后, 速度变成, \n",
    "> $v \\leftarrow \\alpha v + (- \\epsilon g)$\n",
    "\n",
    "$\\alpha \\in [0, 1)$，衡量上次梯度指数衰减变化对当前影响大小. \n",
    "\n",
    "+ 参数移动到新的位置, \n",
    "> $\\theta \\leftarrow \\theta + v$\n",
    "\n",
    "\n",
    "## 1.1 Nesterov动量\n",
    "\n",
    "### (1) 更新参数\n",
    "> $ \\boldsymbol{ \\widetilde {\\theta} } = \\boldsymbol{\\theta} + \\alpha \\boldsymbol{v}$\n",
    "\n",
    "### (2) 计算梯度\n",
    "> $\\boldsymbol{g} = \\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "L(f(\\boldsymbol{x}^{(i)}; \\boldsymbol { \\widetilde { \\theta } }), \\boldsymbol{y}^{(i)})$\n",
    "\n",
    "### (3) 计算动量\n",
    "\n",
    "与原始Momentum施加的力量的方向可能不同, Nesterov先<b>假设</b>按照上一步的速度（方向）移动小球, 再根据新的位置与最优位置的差异来计算移动方向, \n",
    "\n",
    "> $\\boldsymbol{v} = \\alpha \\boldsymbol{v} - \\epsilon \\boldsymbol{g}$\n",
    "\n",
    "### (4) 更新参数\n",
    "> $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} + \\epsilon \\boldsymbol{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 自适应学习率 Adaptive Learning Rate\n",
    "\n",
    "## 2.1 AdaGrad\n",
    "\n",
    "> + 计算梯度： $\\boldsymbol{g}$\n",
    "> + 累积梯度平方：$\\boldsymbol{r} \\gets \\boldsymbol{r} + \\boldsymbol{g} \\odot \\boldsymbol{g}$\n",
    "> + 更新参数：$\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \n",
    "\\frac{\\epsilon}{\\sigma + \\sqrt{\\boldsymbol{r}}} \\odot \\boldsymbol{g}$ ，$\\sigma$是一个很小的常数，如${10}^{-7}$\n",
    "\n",
    "累积梯度平方可能导致学习率下降过快。\n",
    "\n",
    "## 2.2 RMSProp\n",
    "\n",
    "采用梯度指数加权的移动平均来更新学习率，该方法经常被采用。\n",
    "\n",
    "> + 计算梯度：$\\boldsymbol{g}$\n",
    "> + 累积梯度指数加权的滑动平均：$\\boldsymbol{r} \\gets \\rho \\boldsymbol{r} + (1 - \\rho) \\boldsymbol{g} \\odot \\boldsymbol{g} $\n",
    "> + 更新参数：$\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \n",
    "\\frac{\\epsilon}{\\sigma + \\sqrt{\\boldsymbol{r}}} \\odot \\boldsymbol{g}$ ，$\\sigma$是一个很小的常数，如${10}^{-7}$\n",
    "\n",
    "## 2.3 Adam\n",
    "\n",
    "Adaptive Moments\n",
    "\n",
    "$\\rho_1, \\rho_2 \\in [0, 1)$，$\\rho_1$建议为0.9， $\\rho_2$建议为0.999\n",
    "\n",
    "> + $t \\gets t + 1$\n",
    "> + 更新有偏一阶矩：$\\boldsymbol{s} \\gets \\rho_1 \\boldsymbol{s} + (1 - \\rho_1) \\boldsymbol{g}$\n",
    "> + 更新有偏二阶矩：$\\boldsymbol{r} \\gets \\rho_2 \\boldsymbol{r} + (1 - \\rho_2) \\boldsymbol{g} \\odot \\boldsymbol{g}$\n",
    "> + 修正一阶矩偏差：$\\boldsymbol{\\hat{s}} \\gets \\frac{\\boldsymbol{s}}{1 - \\rho_1^t}$\n",
    "> + 修正二阶矩偏差：$\\boldsymbol{\\hat{r}} \\gets \\frac{\\boldsymbol{r}}{1 - \\rho_2^t}$\n",
    "> + 更新参数：$\\boldsymbol{\\theta} \\gets \\boldsymbol{\\theta} - \n",
    "\\epsilon \n",
    "\\frac{\\boldsymbol{\\hat{s}}}{\\sigma + \\sqrt{\\boldsymbol{\\hat{r}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 二阶近似方法 Second Order Methods\n",
    "\n",
    "## 3.1 牛顿法\n",
    "\n",
    "由泰勒展开，\n",
    "\n",
    "> $f(x+\\Delta{x}) \\approx f(x) + \\Delta{x}^{\\top}g + \\frac{1}{2}\\Delta{x}^{\\top}H\\Delta{x}$\n",
    "\n",
    "> $y = f(x+\\Delta{x}) - f(x) \\approx \\Delta{x}^{\\top}g + \\frac{1}{2}\\Delta{x}^{\\top}H\\Delta{x}$\n",
    "\n",
    "> 令，$\\nabla_{\\Delta{x}}y = g + \\frac{1}{2}(H+H^{\\top})\\Delta{x} = g + H\\Delta{x} = 0$，得到\n",
    "\n",
    "> $\\Delta{x} = - H ^{-1}g$\n",
    "\n",
    "所以更新参数规则为，\n",
    "\n",
    "> $x \\gets x - \\epsilon ^ {-1} g$\n",
    "\n",
    "每次迭代需要计算Hessian矩阵（$\\mathbb{R}^{k \\times k}$）的逆，时间复杂度为$O(k^3)$，计算量太大。\n",
    "\n",
    "## 3.2 共轭梯度法\n",
    "\n",
    "## 3.3 BFGS\n",
    "\n",
    "找到$H^{-1}$的近似矩阵。\n",
    "\n",
    "## 3.4 L-BFGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 批标准化 Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
