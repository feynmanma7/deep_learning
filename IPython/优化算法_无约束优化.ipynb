{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>无约束优化</h1>\n",
    "\n",
    "参考文献: \n",
    "\n",
    "[1] Numerical Optimization, Jorge Nocedal & Stephen J.Wright\n",
    "\n",
    "# 0. Line Search\n",
    "\n",
    "在迭代更新参数的过程中, 需要选择参数更新的方向$p$以及更新的步长$\\alpha > 0$, \n",
    "\n",
    "> $x_{k+1} = x_k + \\alpha_k p_k$\n",
    "\n",
    "> $\\alpha_k = \\arg\\min_{\\alpha} f(x_k + \\alpha p_k)$\n",
    "\n",
    "Line Search精确查找步长并不容易，一般采用非精确查找inexact line search.\n",
    "\n",
    "## 0.1 The Wolfe Conditions\n",
    "\n",
    "为了让(损失)函数迭代下降, 有\n",
    "\n",
    "> $f(x_{k+1}) < f(x_k)$, \n",
    "\n",
    "从而满足对于很小的$\\epsilon > 0$, \n",
    "\n",
    "> $f(x_{k+1}) <= f(x_k) - \\epsilon$\n",
    "\n",
    "可以令, \n",
    "\n",
    "> $\\epsilon = - c_1 \\alpha_k \\nabla f_k^T p_k $, 对于常数$0 < c_1 < 1$, \n",
    "\n",
    "从而, \n",
    "> + $f(x_{k+1}) <= f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k = l(\\alpha_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让$f(x_{k+1})$下降, 又直线$l(a_k)$的斜率$\\nabla_{\\alpha_k} l(\\alpha_k) = c_1 \\nabla f_k^T p_k < 0$, 应当有, 函数在当前点的斜率大于$l(a_k)$的斜率, \n",
    "\n",
    "> $\\nabla_{\\alpha_k} f(x_{k+1}) >  \\nabla_{\\alpha_k} l(\\alpha_k) \n",
    "= c_1 \\nabla f_k^T p_k$\n",
    "\n",
    "从而, \n",
    "\n",
    "> + $\\nabla f(x_{k+1})^T p_k >= c_2 \\nabla f_k^T p_k, 0 < c_1 < c_2 < 1$\n",
    "\n",
    "## 0.2 Strong Wolfe Conditions\n",
    "\n",
    "因为$l(\\alpha_k)$斜率小于0, 为了让$f(x_{k+1})$尽可能接近驻点(stationary point, $f'(x_{k+1}) = 0$), 限制条件$f(x_{k+1}) < 0$, \n",
    "\n",
    "> $|\\nabla f(x_{k+1})^T p_k| <= |c_2 \\nabla f_k^T p_k|, 0 < c_1 < c_2 < 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Trust-Region Methods\n",
    "# 2. Conjugate Gradient Methods\n",
    "\n",
    "# 3. Newton Methods\n",
    "\n",
    "由泰勒展开，\n",
    "\n",
    "> $f(x) \\approx f(x_0) + (x - x_0)^{\\top}g + \\frac{1}{2}(x - x_0)^{\\top}H (x - x_0)$\n",
    "\n",
    "> $g = f'(x_0)$\n",
    "\n",
    "> $H = f^{''}(x_0)$\n",
    "\n",
    "对$f(x)$关于$x$求偏导，令其等于0, \n",
    "\n",
    "> $f'(x) = g + H (x - x_0) = 0$\n",
    "\n",
    "得到, \n",
    "\n",
    "> $x^{*} = x_0 - H^{-1} g $\n",
    "\n",
    "\n",
    "所以更新参数规则为，\n",
    "\n",
    "> $x \\leftarrow x_0 - H^{-1} g$\n",
    "\n",
    "每次迭代需要计算Hessian矩阵（$\\mathbb{R}^{k \\times k}$）的逆，时间复杂度为$O(k^3)$，计算量太大。\n",
    "\n",
    "# 4. 二阶优化之拟牛顿法 Quasi-Newton Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
