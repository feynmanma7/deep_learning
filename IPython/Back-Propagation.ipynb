{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Back Propagation</h1>\n",
    "\n",
    "# 0. Multi-Layer Perceptron\n",
    "\n",
    "以多层感知机为例，BP（反向传播）是解决MLP问题的一种方法。\n",
    "\n",
    "## 0.0 网络层\n",
    "\n",
    "> 一共$L+2$层的多层感知机，包含输入层、$L$个隐含层以及输出层。分别定义为第$0$层，第$l\\in\\{1,2,...,L-L\\}$层以及第$L+1$层；\n",
    "\n",
    "> + 输入层：$X \\in \\mathbb{R}^{N \\times M}$，共$N$个样本，每个样本$M$维；\n",
    "\n",
    "> + 隐含层：隐含层$l$的节点个数为$n\\_unit^{(l)}$，输入为$input^{(l)}$，输出为$output^{(l)}$；该层权重定义为上一层节点$i$到当前层节点$j$的权重$w_{ij}^{(l)} \\in \\mathbb{R}^{n\\_unit^{(l-1)} \\times n\\_unit^{(l)}}$，偏置为$b^{(l)} \\in \\mathbb{R}^{n\\_unit^{(l)}}$；\n",
    "\n",
    "> + 输出层：节点个数为$n\\_unit^{(L+1)}$，输入为$input^{(L+1)}$，输出为$output^{(L+1)}$；权重定义为最后一个隐含层的节点$i$到输出层的节点$j$的权重$w_{ij}^{(L+1)} \\in \\mathbb{R}^{n\\_unit^{(L)} \\times n\\_unit^{(L+1)}}$，偏置为$b^{(L+1)} \\in \\mathbb{R}^{n\\_unit^{(L+1)}}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 输入输出\n",
    "\n",
    "> + 输入层：输入层输入、输出一样，都是$X \\in \\mathbb{R}^{N \\times M}$;\n",
    "\n",
    "> + 隐含层：第$l$层第$i$个节点输入为$input_i^{(l)} = \\sum_j^{n\\_unit^{(l-1)}} w_{ji}^{(l)} output_j^{(l-1)} + b^{(l)}$，输出为$output_i^{(l)} = activation(input_i^{(l)})$；\n",
    "\n",
    "> + 输出层：第$i$个节点输入为$input_i^{(L+1)} = \\sum_j^{n\\_unit^{(L)}} w_{ji}^{(L+1)} output_j^{(L)} + b^{(L+1)}$，输出为$output_i^{(L+1)} = activation(input_i^{(L+1)})$；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forward Propagation\n",
    "\n",
    "对于mini_batch数据$X \\in \\mathbb{R}^{N \\times M}$，前向传播得到所有隐含层以及输出层的输入输出。\n",
    "\n",
    "考虑向量计算形式，对于$l \\in \\{1, 2, ..., L, L+1\\}$，\n",
    "\n",
    "> + $output^{(0)} = X$ \n",
    "\n",
    "> + $input^{(l)} = output^{(l-1)} \\times w^{(l)} + b^{(l)}$\n",
    "\n",
    "> + $output^{(l)} = activation(input^{(l)})$\n",
    "\n",
    "激活函数选择sigmoid函数，$\\frac{\\partial{activation(x)}}{\\partial{x}} = activation(x) * (1 - activation(x))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Back Propagation Error\n",
    "\n",
    "不管是分类还是回归问题，训练的时候如果预测与预期（真实值）不符，就需要调整网络的权重。对于网络中的每一条边，都需要根据最终的误差进行微调，从而需要计算最终误差关于网络权重的偏导数。\n",
    "\n",
    "$N$个样本的mini_batch平均误差：\n",
    "> $\\Delta E = \\frac{1}{N}\\sum_{n=1}^{N} \\Delta{E_n}$\n",
    "\n",
    "> $ = \\frac{1}{N}\\sum_{n=1}^{N} \\left(\\frac{1}{2} \\sum_{j=1}^{n\\_unit^{(L+1)}} (y_{nj} - t_{nj})^2\\right)$\n",
    "\n",
    "考虑向量计算形式，对于$l \\in \\{L+1, L, ..., 2, 1\\}$，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 输出层\n",
    "\n",
    "+ 最后一个隐含层的节点$i$到输出层的节点$j$之间的权重$w_{ij}^{(L+1)}$，\n",
    "\n",
    "> $\\frac{\\partial{\\Delta E}} {\\partial{w_{ij}^{(L+1)}}} \n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial{\\Delta E_n}} {\\partial{w_{ij}^{(L+1)}}} $\n",
    "   \n",
    "> 令 $\\delta_j^{(l)} = \\frac{\\partial{\\Delta E_n}} {\\partial{input_j^{(l)}}} $ ,\n",
    "\n",
    "> $\\delta_j^{(L+1)} = \\frac{\\partial{\\Delta E_n}} {\\partial{input_j^{(L+1)}}}\n",
    "= (y_j^{(L+1)} - t_j) * \\sigma^{'}(input_j^{(L+1)})\n",
    "= (y_j^{(L+1)} - t_j) * \\left( output_j^{(L+1)} (1 - output_j^{(L+1)}) \\right)$\n",
    "\n",
    "> $\\frac{\\partial{\\Delta E_n}} {\\partial{w_{ij}^{(L+1)}}}\n",
    "= \\frac{\\partial{\\Delta E_n}} {\\partial{input_j^{(L+1)}}}\n",
    "\\frac{\\partial{input_j^{(L+1)}}} {\\partial{w_{ij}^{(L+1)}}}$\n",
    "\n",
    "> $ = \\delta_j^{(L+1)} * ouput_i^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{\\Delta E}} {\\partial{w_{ij}^{(L+1)}}}\n",
    " = \\frac{1}{N} \\sum_{n}^{N} \\frac{\\partial{\\Delta E_n}} {\\partial{w_{ij}^{(L+1)}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 第$i$个输出节点的偏置$b_i^{(L+1)}$ \n",
    "\n",
    "> $\\frac{\\partial{\\Delta E_n}} {\\partial{b_i^{(L+1)}}} \n",
    "= \\frac{\\partial{\\Delta E_n}} {\\partial{input_j^{(L+1)}}}\n",
    "\\frac{\\partial{input_j^{(L+1)}}} {\\partial{b_i^{(L+1)}}}$\n",
    "\n",
    "> $ = \\delta_j^{(L+1)} * 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{\\Delta E}} {\\partial{b_i^{(L+1)}}} \n",
    "= \\frac{1}{N} \\sum_n^N \\frac{\\partial{\\Delta E_n}} {\\partial{b_i^{(L+1)}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 隐含层\n",
    "\n",
    "+ 隐含层$l-1$的节点$i$到$l$层的节点$j$之间的权重$w_{ij}^{(l)}$，\n",
    "\n",
    "> $\\frac{\\partial{\\Delta E_n}} {\\partial{w_{ij}^{(l)}}}\n",
    "= \\frac{\\partial{\\Delta E_n}} {\\partial{input_j^{(l)}}} \n",
    "\\frac{\\partial{input_j^{(l)}}} {\\partial{w_{ij}^{(l)}}}\n",
    "= \\delta_j^{(l)} * output_i^{(l-1)}$\n",
    "\n",
    "> $\\delta_j^{(l)} =\n",
    "\\frac{\\partial{\\Delta E}} {\\partial{input_j^{(l)}}} \n",
    "= \\sum_{k=1}^{n\\_unit^{(l+1)}} \n",
    "\\frac{\\partial{\\Delta E}} {\\partial{input_k^{(l+1)}}} \n",
    "\\frac{{\\partial{input_k^{(l+1)}}}} {{\\partial{input_j^{(l)}}}}$\n",
    "\n",
    "> $= \\sum_{k=1}^{n\\_unit^{(l+1)}} \n",
    "\\delta_k^{(l+1)}\n",
    " \\left(w_{kj}^{(l+1)} * \\sigma^{'}(input_j^{(l)})\\right)$   \n",
    "\n",
    "> $= \\sigma^{'}(input_j^{(l)})  \\sum_{k=1}^{n\\_unit^{(l+1)}} \n",
    "\\delta_k^{(l+1)}  w_{kj}^{(l+1)}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{\\Delta E}} {\\partial{w_{ij}^{(l)}}} = \n",
    "\\frac{1}{N} \\sum_{n}^{N} \\frac{\\partial{\\Delta E_n}} {\\partial{w_{ij}^{(l)}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 隐含层$l$第$i$个节点的偏置\n",
    "\n",
    "> $\\frac{\\partial{\\Delta E_n}} {\\partial{b_i^{(l)}}} \n",
    "= \\frac{\\partial{\\Delta E_n}} {\\partial{input_j^{(l)}}}\n",
    "\\frac{\\partial{input_j^{(l)}}} {\\partial{b_i^{(l)}}}$\n",
    "\n",
    "> $ = \\delta_j^{(l)} * 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{\\Delta E}} {\\partial{b_i^{(l)}}} \n",
    "= \\frac{1}{N} \\sum_n^N \\frac{\\partial{\\Delta E_n}} {\\partial{b_i^{(l)}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Update Weights\n",
    "\n",
    "## 2.0 隐含层\n",
    "\n",
    "+ 第$l$层权重$w_{ij}^{(l)}$，连接第$l-1$层的节点$i$与第$l$层的节点$j$，$l\\in\\{1,2,...,L\\}$\n",
    "\n",
    "> $w_{ij}^{(l)} = w_{ij}^{(l)} - \\eta * \\frac{\\partial{\\Delta E}} {\\partial{w_{ij}^{(l)}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 第$l$层节点$i$的偏置$b_i^{(l)}$\n",
    "\n",
    "> $b_i^{(l)} = b_i^{(l)} - \\eta * \\frac{\\partial{\\Delta E}} {\\partial{b_i^{(l)}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 输出层\n",
    "\n",
    "+ 输出层权重$w_{ij}^{(L+1)}$，连接第$L$层的节点$i$与输出层的节点$j$，\n",
    "\n",
    "> $w_{ij}^{(L+1)} = w_{ij}^{(L+1)} - \\eta * \\frac{\\partial{\\Delta E}} {\\partial{w_{ij}^{(L+1)}}}\n",
    "$\n",
    "\n",
    "+ 输出层节点$i$的偏置$b_i^{(L+1)}$\n",
    "\n",
    "> $b_i^{(L+1)} = b_i^{(L+1)} - \\eta * \\frac{\\partial{\\Delta E}} {\\partial{b_i^{(L+1)}}}$\n",
    "\n",
    "\n",
    "## 2.2 动态规划\n",
    "\n",
    "因为在每一次mini_batch自第一个隐含层开始，向后直到输出层更新网络权重参数的过程，需要计算误差对每一条边的偏导数。隐含层的权重更新需要计算误差对隐含层输入的偏导$\\delta^{(l)}$，从上面的推导中看出，偏导依赖后面一层的偏导$\\delta^{(l+1)}$。\n",
    "\n",
    "为减少大量重复计算，使用<b>动态规划</b>算法，在每一次的mini_batch学习中，记录每一条样本的误差对每个输入层（隐含层以及输出层）的偏导。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
