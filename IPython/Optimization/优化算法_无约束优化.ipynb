{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>无约束优化</h1>\n",
    "\n",
    "参考文献: \n",
    "\n",
    "[1] Numerical Optimization, Jorge Nocedal & Stephen J.Wright.\n",
    "\n",
    "[2] A review of trust region algorithms for optimization, Yuan Ya-Xiang.\n",
    "\n",
    "# 0. Line Search\n",
    "\n",
    "在迭代更新参数的过程中, 需要选择参数更新的方向$p$以及更新的步长$\\alpha > 0$, \n",
    "\n",
    "> $x_{k+1} = x_k + \\alpha_k p_k$\n",
    "\n",
    "> $\\alpha_k = \\arg\\min_{\\alpha} f(x_k + \\alpha p_k)$\n",
    "\n",
    "Line Search精确查找步长并不容易，一般采用非精确查找inexact line search.\n",
    "\n",
    "## 0.1 The Wolfe Conditions\n",
    "\n",
    "为了让(损失)函数迭代下降, 有\n",
    "\n",
    "> $f(x_{k+1}) < f(x_k)$, \n",
    "\n",
    "从而满足对于很小的$\\epsilon > 0$, \n",
    "\n",
    "> $f(x_{k+1}) <= f(x_k) - \\epsilon$\n",
    "\n",
    "可以令, \n",
    "\n",
    "> $\\epsilon = - c_1 \\alpha_k \\nabla f_k^T p_k $, 对于常数$0 < c_1 < 1$, \n",
    "\n",
    "从而, \n",
    "> + $f(x_{k+1}) <= f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k = l(\\alpha_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让$f(x_{k+1})$下降, 又直线$l(a_k)$的斜率$\\nabla_{\\alpha_k} l(\\alpha_k) = c_1 \\nabla f_k^T p_k < 0$, 应当有, 函数在当前点的斜率大于$l(a_k)$的斜率, \n",
    "\n",
    "> $\\nabla_{\\alpha_k} f(x_{k+1}) >  \\nabla_{\\alpha_k} l(\\alpha_k) \n",
    "= c_1 \\nabla f_k^T p_k$\n",
    "\n",
    "从而, \n",
    "\n",
    "> + $\\nabla f(x_{k+1})^T p_k >= c_2 \\nabla f_k^T p_k, 0 < c_1 < c_2 < 1$\n",
    "\n",
    "## 0.2 Strong Wolfe Conditions\n",
    "\n",
    "因为$l(\\alpha_k)$斜率小于0, 为了让$f(x_{k+1})$尽可能接近驻点(stationary point, $f'(x_{k+1}) = 0$), 限制条件$f(x_{k+1}) < 0$, \n",
    "\n",
    "> + $|\\nabla f(x_{k+1})^T p_k| <= |c_2 \\nabla f_k^T p_k|, 0 < c_1 < c_2 < 1$\n",
    "\n",
    "## 0.3 Line Search Methods\n",
    "\n",
    "> + 初始化$\\alpha_k, p_k$\n",
    "\n",
    "> + 检查$\\alpha_k$是否满足Strong Wolfe Conditions, 如果不满足, $\\alpha_k \\leftarrow \\rho \\alpha_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Trust-Region Methods\n",
    "\n",
    "迭代逼近函数最优值(或局部最优)的过程中,考虑两个因素：\n",
    "\n",
    "> + 一个局部逼近模型\n",
    "\n",
    "> + 一个置信域(超曲面)\n",
    "\n",
    "使得在置信域内, 局部逼近模型对原始函数（如损失函数）有好的逼近, 我们的目标是寻找好的<b>局部逼近模型</b>以及最优的<b>置信域</b>. \n",
    "\n",
    "> $\\min_{p} m_k (p)$\n",
    "\n",
    "> $s.t. ||p|| <= \\Delta_k$\n",
    "\n",
    "对$f(x_k + p)$进行泰勒展开, \n",
    "\n",
    "> $f(x_k + p) = f_k + g_k^T p + \\frac{1}{2} p^T \\nabla^2 f(x_k + tp) p$\n",
    "\n",
    "逼近模型选择使用泰勒展开, \n",
    "\n",
    "> $m_k (p) = f_k + g_k^T p + \\frac{1}{2} p^T B_k p$\n",
    "\n",
    "$B_k$是一个对称矩阵."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑预测减少($m_k(0) - m_k(p_k)$)与实际减少($f(x_k) - f(x_k + p_k)$)的一致性, \n",
    "\n",
    "> $\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}$\n",
    "\n",
    "如果比率$\\rho_k$接近1, 说明已经找到合适的步长$p_k$, 更新参数$x_{k+1} = x_k + p_k$;\n",
    "\n",
    "如果比率$\\rho_k$比较小, 说明目前的步长可能太大（局部逼近模型只是在当前点的局部逼近效果比较好, 远离当前点可能已经是其它的数据分布了）, 需要缩减步长; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Conjugate Gradient Methods\n",
    "\n",
    "采用Line Search的Steepest Descent面临收敛比较慢的问题:\n",
    "\n",
    "如果数据有多个维度, 在第$t$次迭代时移动的方向为$d_t$, 当Line Search停止的时候, 已经找到局部最小点, 函数在$d_t$这个方向的方向导数等于0, $\\nabla_{\\theta}f(\\theta) d_t  = 0$; \n",
    "\n",
    "从而第$t+1$次迭代的时候移动的方向$d_{t+1} = \\nabla_{\\theta}f(\\theta)$与$d_t$完全正交; 然而沿着方向$d_{t+1}$移动, 不能确保移动到新位置在方向$d_t$上依然保持最小值, 浪费了刚刚在该方向上进行的搜索;\n",
    "\n",
    "## 2.0 Motivation\n",
    "\n",
    "对函数的等高线进行正交分解, 对$n$个正交方向, 依次在正交方向进行搜索; 每一次都对当前以及上一个正交方向同时进行搜索;\n",
    "\n",
    "> + 对基$\\{a_1, a_2, ..., a_n\\}$为的数据进行正交分解, 找到正交基$\\{b_1, b_2, ..., b_n \\}$; \n",
    "\n",
    "> + 将数据映射到正交基上; \n",
    "\n",
    "> + 依次在每一个正交基上进行Line Search, 只需要$n$次迭代, 就找到了最优位置;\n",
    "\n",
    "> + 将最优位置再映射回原来的基上去. \n",
    "\n",
    "> <b>What a brilliant idea!</b>\n",
    " \n",
    "\n",
    "## 2.1 Gram–Schmidt \n",
    "\n",
    "## 2.2 Conjugate\n",
    "\n",
    "对于非零向量集合$\\{p_1, p_2, ..., p_N\\}$以及对称正定矩阵$A$, 如果有, \n",
    "\n",
    "> $p_i^T A p_j = 0, i \\neq j$\n",
    "\n",
    "则称向量$p_i$与$p_j$关于矩阵$A$共轭. \n",
    "\n",
    "## 2.3 Conjugate Gradients\n",
    "\n",
    "> $d_{t+1} = \\nabla_{\\theta} f(\\theta) + \\beta_t d_t $\n",
    "\n",
    "\n",
    "# 3. Newton Methods\n",
    "\n",
    "由泰勒展开，\n",
    "\n",
    "> $f(x) \\approx f(x_0) + (x - x_0)^{\\top}g + \\frac{1}{2}(x - x_0)^{\\top}H (x - x_0)$\n",
    "\n",
    "> $g = f'(x_0)$\n",
    "\n",
    "> $H = f^{''}(x_0)$\n",
    "\n",
    "对$f(x)$关于$x$求偏导，令其等于0, \n",
    "\n",
    "> $f'(x) = g + H (x - x_0) = 0$\n",
    "\n",
    "得到, \n",
    "\n",
    "> $x^{*} = x_0 - H^{-1} g $\n",
    "\n",
    "\n",
    "所以更新参数规则为，\n",
    "\n",
    "> $x \\leftarrow x_0 - H^{-1} g$\n",
    "\n",
    "每次迭代需要计算Hessian矩阵（$\\mathbb{R}^{k \\times k}$）的逆，时间复杂度为$O(k^3)$，计算量太大。\n",
    "\n",
    "# 4. 二阶优化之拟牛顿法 Quasi-Newton Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
