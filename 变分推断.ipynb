{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>变分推断 Variational Inference</h1>\n",
    "\n",
    "<b>参考资料</b>\n",
    "\n",
    "[1] An Introduction to Variational Methods for Graphical Models, Jordan etc. 1999.\n",
    "\n",
    "[2] Variational Inference- Foundations and Modern Methods, Blei etc. NIPS Tutorial, 2016.\n",
    "\n",
    "[3] Pattern Recognition and Machine Learning, Chapter 10, Bishop, 2006.\n",
    "\n",
    "[4] Variational Inference: A Review for Statisticians, Blei etc. 2017.\n",
    "\n",
    "\n",
    "# 0. 概率机器学习\n",
    "\n",
    "## 0.0 概率模型\n",
    "\n",
    "给定可观测变量$x$与隐含变量$z$， 概率模型考虑$x$与$z$的联合分布，\n",
    "\n",
    "> $p(x, z)$\n",
    "\n",
    "## 0.1 推断\n",
    "\n",
    "给定观测变量，推断隐含变量$z$的后验概率分布$p(z|x)$，\n",
    "\n",
    "> $p(z|x) = \\frac{p(x, z)}{p(x)}$\n",
    "\n",
    "给定$z$之后，$p(x, z)$的计算是比较容易的；如在GMM中知道每个样本点属于每个类的概率，则可以计算完整数据$(x, z)$的似然。\n",
    "\n",
    "但是不完整数据$x$的似然$p(x) = \\int_z p(x, z) dz$，是难以计算的；如在GMM中，需要遍历每个样本点属于每个类的概率空间。\n",
    "\n",
    "因此需要采用一些近似推断的方式来计算$p(x)$，一般包括（1）抽样方法；（2）变分推断。本文关注<b>变分推断</b>。\n",
    "\n",
    "# 1. 变分推断 Variational Inference\n",
    "\n",
    "变分推断是一个确定性的近似推断方式，在有限元分析、量子力学、统计力学以及统计学等领域有着广泛的应用。\n",
    "\n",
    "基本思想：将复杂问题（intratable）转变为简单问题，降低原始问题的自由度(degrees of freedom)。\n",
    "\n",
    "# 1.0 ELBO (Evidence Lower BOund)\n",
    "\n",
    "隐变量$Z$的后验分布$p(Z|X)$难以计算（intractable），用一个好计算的分布$q(Z)$来尽可能接近$p(Z|X)$。即，最小化KL散度，\n",
    "\n",
    "> $q^*(Z) = \\arg\\max_{q(Z) \\in \\mathcal{A}} \\mathbb{KL}[q(Z)||p(Z|X)] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $ \\mathbb{KL}[q(Z)||p(Z|X)] $\n",
    "\n",
    "> $ = \\mathbb{E}_q[\\log q(Z) - \\log p(Z|X)]$\n",
    "\n",
    "> $ = \\mathbb{E}_q[\\log q(Z) - \\log p(Z, X)] + \\log p(X)$\n",
    "\n",
    "> $ = \\mathbb{E}_q[\\log q(Z)] - \\mathbb{E}_q[\\log p(Z, X)] + \\log p(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> $ \\log p(X) = \\mathbb{KL}[q(Z)||p(Z|X)] +\n",
    "\\underbrace{\n",
    "\\mathbb{E}_q[\\log p(Z, X)] - \\mathbb{E}_q[\\log q(Z)]}_{ELBO(q)}$\n",
    "\n",
    "> $ = \\mathbb{KL}[q(Z)||p(Z|X)] + ELBO(q)$\n",
    "\n",
    "> $ \\geqslant ELBO(q)$\n",
    "\n",
    "最小化KL散度，等价于最大化$ELBO(q)$，可以让$ELBO(q)$更接近$\\log p(X)$，此时$q(Z) = p(Z|X)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> $ ELBO(q) = \\mathbb{E}_q[\\log p(Z, X)] - \\mathbb{E}_q[\\log q(Z)] $\n",
    "\n",
    "> $ = \\mathbb{E}_q[\\log p(X|Z)] + \\mathbb{E}_q[\\log p(Z)] - \\mathbb{E}_q[\\log q(Z)]$\n",
    "\n",
    "> $ = \\mathbb{E}_q[\\log p(X|Z)] - \\mathbb{KL}[q(Z)||p(Z)] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 The Mean-field Variational Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
