{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word2vec</h1>\n",
    "\n",
    "Preference\n",
    "\n",
    "[1] word2vec Parameter Learning Explained.\n",
    "\n",
    "\n",
    "# 0. CBOW\n",
    "\n",
    "## 0.0 One-word Context CBOW\n",
    "\n",
    "### Input\n",
    "\n",
    "One-hot variable $\\boldsymbol{x}(n\\_V, 1)$, assuming $x_k=1; x_{k'}=0, k' \\ne k$, represents the $k$-th word of the vocabulary of size $n\\_V$.\n",
    "\n",
    "### Hidden \n",
    "\n",
    "One hidden layer $h$. $W(n\\_V, n\\_h)$ is the weight matrix connecting the input and hidden layer.\n",
    "\n",
    "Given an one word context $w_I$, the \n",
    "\n",
    "> $h = W^T \\boldsymbol{x}$\n",
    "\n",
    "> $= W^T_{(k, \\centerdot)}$\n",
    "\n",
    "> $:= v^T_{w_I}$\n",
    "\n",
    "The value of hidden layer $h(n\\_h, 1)$ can be defined as the represention vector of the input context word $w_I$.\n",
    "\n",
    "### Output\n",
    "\n",
    "The output layer is a softmax value list of each word in the vocabulary.\n",
    "\n",
    "$W'(n\\_h, n\\_V)$ is the weight matrix connecting the hidden and output layer. \n",
    "\n",
    "The unnomalized $j$-the value of output $u_j$ is, \n",
    "\n",
    "> $u_j = v'^T_{w_j} h$\n",
    "\n",
    "$v'_{w_j}(n\\_h, 1)$ is the $j$-th column of $W'$.\n",
    "\n",
    "+ Softmax\n",
    "\n",
    "For input context word $w_I$ and output word $w_j$, the conditional posterior distribution is, \n",
    "\n",
    "> $p(w_j|w_I) = \\frac{\\exp(u_j)}{\\sum_{j'=1}^V \\exp(u_j')}$\n",
    "\n",
    "> $= \\frac{\\exp(v'^T_{w_j} v_{w_I})}\n",
    "{\\sum_{j'}^{V} \\exp(v'^T_{w_{j'}} v_{w_I})} = y_j$\n",
    "\n",
    "$y_j$ is the $j$-th value of the output layer.\n",
    "\n",
    "### Objective function\n",
    "\n",
    "For one training sample, for input context word $w_I$ and the actual output word $w_O$, the training objective is to maximize the conditional posterior distribution.\n",
    "\n",
    "> $\\max p(w_O|w_I) = \\max y_{j^*}$\n",
    "\n",
    "> $=\\max \\log y_{j^*}$\n",
    "\n",
    "> $= \\max u_{j^*} - \\log \\sum_{j'=1}^V \\exp (u_{j'})$\n",
    "\n",
    "> $:= -J$\n",
    "\n",
    "Thus, $J = - \\log p(w_O|w_I)$ is our loss function.\n",
    "\n",
    "### SGD\n",
    "\n",
    "We need to learn to adjust $W$ and $W'$ which are actually the input and output vectors of words separately.\n",
    "\n",
    "+ $W'$\n",
    "\n",
    "> $\\frac{\\partial{J}}{\\partial{u_j}} = y_j - \\mathbb{1}(j=j^*)\n",
    ":= e_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{J}}{\\partial{w^{'}_{ij}}} = \n",
    "\\frac{\\partial{J}}{\\partial{u_j}} \\centerdot \n",
    "\\frac{\\partial{u_j}}{\\partial{w^{'}_{ij}}}\n",
    "= e_j \\centerdot h_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    "> ${w^{'}_{ij}}^{(new)} = {w^{'}_{ij}}^{(old)} - \\eta \\centerdot e_j \\centerdot h_i$\n",
    "\n",
    "Or, \n",
    "\n",
    "> ${v^{'}_{w_j}}^{(new)} = {v^{'}_{w_j}}^{(old)} - \\eta \\centerdot e_j \\centerdot \\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $W$\n",
    "\n",
    "> $\\frac{\\partial{J}}{\\partial{w_{ki}}} = \n",
    "\\frac{\\partial{J}}{\\partial{h_i}} \\centerdot\n",
    "\\frac{\\partial{h_i}}{\\partial{w_{ki}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{J}}{\\partial{h_i}} = \n",
    "\\sum_{j=1}^{n\\_V}\n",
    "\\frac{\\partial{J}}{\\partial{u_j}} \\centerdot\n",
    "\\frac{\\partial{u_j}}{\\partial{h_i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{u_j}}{\\partial{h_i}} = w^{'}_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{h_i}}{\\partial{w_{ki}}} = x_k$\n",
    "\n",
    "> $h_i = \\sum_{k=1}^{n\\_V} x_k \\centerdot w_{ki}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$\n",
    "\n",
    "> $\\frac{\\partial{J}}{\\partial{w_{ki}}} = \n",
    "(\\sum_{j=1}^{n\\_V} e_j \\centerdot w^{'}_{ij}) \\centerdot x_k  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    "> ${w_{ki}}^{(new)} = {w_{ki}}^{(old)} - \\eta (\\sum_{j=1}^{n\\_V} e_j \\centerdot w^{'}_{ij}) x_k $\n",
    "\n",
    "Or, \n",
    "\n",
    "> ${v_{w_I}}^{(new)} = {v_{w_I}}^{(old)} - \\eta EH^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Multi-word Context CBOW\n",
    "\n",
    "+ Input layer, multi-word\n",
    "Compared to one-word context CBOW, the context here are $C$ words $\\{\\boldsymbol{x_1}, \\boldsymbol{x_2}, ..., \\boldsymbol{x_C} \\}$.\n",
    "\n",
    "+ Hidden layer, averaged\n",
    "The average value of the projections of $C$ input context words is defined as the value hidden layer.\n",
    "\n",
    "> $h = \\frac{1}{C} \\sum_{i=1}^C W^T \\boldsymbol{x_i}$\n",
    "\n",
    "+ Update weights, $W_1$\n",
    "\n",
    "The update rule for $W_1$ keeps the same, except that the value of $h$ is changed.\n",
    "\n",
    "> ${v^{'}_{w_j}}^{(new)} = {v^{'}_{w_j}}^{(old)} - \\eta \\centerdot e_j \\centerdot \\boldsymbol{h}$\n",
    "\n",
    "+ Update weights, $W$\n",
    "\n",
    "Update weights for $C$ input context words rather than only one word in the One-word Context CBOW.\n",
    "\n",
    "> ${v_{w_{I, c}}}^{(new)} = {v_{w_{I, c}}}^{(old)} - \\frac{1}{C} \\centerdot \\eta \\centerdot EH^T,  c = \\{1, 2, ..., C\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Skip-Gram\n",
    "\n",
    "Give only one input word, predict the context output words with one-side size is $C$.\n",
    "\n",
    "+ Input Layer\n",
    "\n",
    "Only one input word $\\boldsymbol{x}$.\n",
    "\n",
    "+ Hidden Layer\n",
    "\n",
    "Like the one word input context in CBOW, the hidden layer $h$ is, \n",
    "\n",
    "> $\\boldsymbol{h} = W^T \\boldsymbol{x}$\n",
    "\n",
    "For the input word $w_I$, the value of hidden layer is, \n",
    "\n",
    "> $\\boldsymbol{h} = W^T_{(w_I, )} := v^T_{w_I}$\n",
    "\n",
    "+ Output Layer\n",
    "\n",
    "For the $C$ output context words, \n",
    "\n",
    "> $p(w_{c, j} = w_{O, c}|w_I) = y_{c, j}$\n",
    "> $= \\frac{\\exp(u_{c, j})}{\\sum_{j=1}^{n\\_V} \\exp(u_{j'})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $u_{c, j} = u_j = {v'}^T_{w_j} \\centerdot \\boldsymbol{h}, c = \\{1, 2, ..., C\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Loss function\n",
    "\n",
    "> $J = - \\log p(w_{O, 1}, w_{O, 2}, ..., w_{O, C}| w_I) $\n",
    "\n",
    "> $= - \\log \\prod_{c=1}^C p(w_{O, c}|w_I)$\n",
    "\n",
    "> $= - \\log \\prod_{c=1}^C \\frac{\\exp(u_{c, j^*_c})}{\\sum_{j'=1}^{n\\_V}\\exp(u_{j'})}$\n",
    "\n",
    "> $= - \\sum_{c=1}^C u_{j^*_c} + C \\centerdot \\log \n",
    "\\sum_{j'=1}^{n\\_V} \\exp(u_{j'})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Update Weights, $W_1$\n",
    "\n",
    "> $\\frac{\\partial{J}}{\\partial{u_{c, j}}} = \n",
    "C \\centerdot y_j - 1 := EI_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{J}}{\\partial{w^{'}_{ij}}} = \n",
    "\\frac{\\partial{J}}{\\partial{u_j}} \\centerdot\n",
    "\\frac{\\partial{u_j}}{\\partial{w^{'}_{ij}}} = \n",
    "EI_j \\centerdot h_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ${w^{'}_{ij}}^{(new)} = {w^{'}_{ij}}^{(old)} - \n",
    "\\eta \\centerdot EI_j \\centerdot h_i$\n",
    "\n",
    "Or, \n",
    "\n",
    "> ${v^{'}_{w_j}}^{(new)} = {v^{'}_{w_j}}^{(old)} - \\eta \\centerdot EI_j \\centerdot \\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Update Weights, W\n",
    "\n",
    "> $\\frac{\\partial{J}}{\\partial{h_i}} = \n",
    "\\sum_{j=1}^{n\\_V} \\frac{\\partial{J}}{\\partial{u_j}} \\centerdot \n",
    "\\frac{\\partial{u_j}}{\\partial{h_i}} = \n",
    "\\sum_{j=1}^{n\\_V} EI_j \\centerdot w^{'}_{ij} := EH_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial{J}}{\\partial{w_{ki}}} = \n",
    "\\frac{\\partial{J}}{\\partial{h_i}} \\centerdot\n",
    "\\frac{\\partial{h_i}}{\\partial{w_{ki}}} = EH_i \\centerdot x_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ${w_{ki}}^{(new)} = {w_{ki}}^{(old)} - \\eta \\centerdot EH_i \\centerdot x_k$\n",
    "\n",
    "Or, \n",
    "\n",
    "> ${v_{w_I}}^{(new)} = {v_{w_I}}^{(old)} - \\eta \\centerdot EH^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
