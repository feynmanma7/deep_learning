{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>贝叶斯机器学习 Bayesian Machine Learning</h1>\n",
    "\n",
    "参考资料:\n",
    "\n",
    "[1]<a href=\"https://metacademy.org/roadmaps/rgrosse/bayesian_machine_learning\">https://metacademy.org/roadmaps/rgrosse/bayesian_machine_learning</a>\n",
    "\n",
    "[2] PRML\n",
    "\n",
    "# 0. 主要问题\n",
    "\n",
    "## 0.0 参数估计 Parameter Estimation\n",
    "\n",
    "给定观测数据，计算模型参数的后验概率；从而可以利用计算好的参数进行模型预测；\n",
    "\n",
    "\n",
    "## 0.1 模型比较 Model Comparison\n",
    "\n",
    "如果得到了多个模型，应该选择哪一个模型。\n",
    "\n",
    "可以使用<b>贝叶斯模型平均</b>，而不仅仅使用某一个单独的模型。对得到的模型定义先验，然后使用模型的后验的平均结果来进行预测。\n",
    "\n",
    "# 1. 非贝叶斯方法\n",
    "\n",
    "## 1.1. 极大似然估计 Maximum Likelihood Estimation\n",
    "\n",
    "比如有某个地区一批学生的身高数据，想知道该地区学生的生长发育情况。\n",
    "假设学生的身高服从正态分布$X\\sim \\mathcal{N}(\\mu,\\sigma^2)$。\n",
    "问题变成估计样本的均值$\\mu$与方差$\\sigma^2$。\n",
    "\n",
    "$X$的似然函数为，\n",
    "> $L(X;\\mu,\\sigma^2) = \\prod_{n=1}^{N} \\mathcal{N}(x_n; \\mu,\\sigma^2) $\n",
    "\n",
    "防止似然函数下溢，计算对数似然，\n",
    "\n",
    "> $l(X;\\mu,\\sigma^2) = \\log L(X;\\mu,\\sigma^2) =\n",
    "\\log \\prod_{n=1}^{N} \\mathcal{N}(x_n; \\mu,\\sigma^2) $ \n",
    "> $ = \\sum_{n=1}^{N} \\log \\mathcal{N}(x_n; \\mu,\\sigma^2)$\n",
    "\n",
    "对数似然$l(X;\\mu,\\sigma^2)$关于$\\mu$与$\\sigma^2$的二阶导小于0，从而$l$存在极大值，为$l$关于$\\mu$与$\\sigma^2$的一阶导。\n",
    "\n",
    "综上，MLE的目标：\n",
    "\n",
    "> + 计算 $\\mathbb{E}(\\ln p(X|\\theta))$；\n",
    "> + 得到最优解 $\\theta^{*} = \\arg\\max_{\\theta}(\\mathbb{E})$\n",
    "\n",
    "## 1.2. 正则化\n",
    "\n",
    "## 1.3. EM算法\n",
    "\n",
    "### 1.3.1. EM\n",
    "\n",
    "如果模型中有与样本相关的隐含变量，则无法直接使用MLE。以K-Means方法为例，每个样本都属于$K$个类中的一个，但属于具体哪一个是未知的，从而也无法收集每个类的数据，对每个类计算聚类中心。\n",
    "\n",
    "但是，如果隐含变量变成已知了，则可以使用MLE方法。\n",
    "\n",
    "目标：计算最优解$\\theta^{*} = \\arg\\max_{\\theta}(\\mathbb{E}(\\ln p(X|\\theta)))$；\n",
    "\n",
    "问题：因为隐变量$Z$的存在，$\\mathbb{E}(\\ln p(X|\\theta))$无法直接计算；如果$p(Z|X, \\theta)$已知，则$\\ln p(X, Z|\\theta) = \\ln \\{p(X|Z, \\theta) p(Z|X, \\theta)\\}$易于计算；\n",
    "\n",
    "方法：$\\mathbb{E}(\\ln p(X|\\theta)) \n",
    "= \\mathbb{E}_{Z}(\\ln p(X, Z| \\theta)) \n",
    "= \\int_{Z} p(Z|X, \\theta) \\ln p(X, Z| \\theta)dZ$，从而问题演变为计算隐变量的后验概率；\n",
    "\n",
    "> + E-步：计算隐含变量（如每一个样本属于每个类的概率）的后验概率$p(Z|X, \\theta^{old})$，计算完整数据关于隐变量的期望\n",
    "$\\mathcal{Q}(\\theta, \\theta^{old}) \n",
    "= \\mathbb{E}_{Z}(\\ln p(X, Z|\\theta, \\theta^{old})\n",
    "= \\int_{Z} p(Z|X, \\theta^{old}) \\log p(X, Z|\\theta)  dZ$；\n",
    "\n",
    "> + M-步：最大化数据集期望$\\mathcal{Q}(\\theta, \\theta^{old})$，得到更优参数\n",
    "$\\theta^{*} = \\arg\\max_{\\theta} \\mathcal{Q}(\\theta, \\theta^{old})$；\n",
    "\n",
    "以GMM为例，\n",
    "\n",
    "> + E-步：计算隐含变量（每一个样本属于每个高斯分布的概率，softmax(概率密度值)）的后验概率，计算数据集关于隐含变量的期望\n",
    "$\\mathbb{E}_{Z}( \\ln p(X, Z|\\mu,\\sigma,\\mu^{old}, \\sigma^{old} ) )$；\n",
    "\n",
    "> + M-步：通过最大化期望得到$\\mu$与$\\sigma$的极大值；\n",
    "\n",
    "\n",
    "### 1.3.2. MM \n",
    "\n",
    "根据需要寻找目标函数的最大值或者最小值被称为Minorize-Maxmization或者Majorize-Minimization。EM是MM的一个特例。\n",
    "\n",
    "以寻找最大值为例，如果目标函数$f(\\theta)$不可微，无法直接求极值。使用Minorize-Maxmization，寻找<b>替代函数</b>$g$，在第$m$步$g(\\theta|\\theta^{m})$有：\n",
    "\n",
    "> + $\\forall \\theta, g(\\theta|\\theta^{m}) <= f(\\theta)$\n",
    "> + $g(\\theta^{m}|\\theta^{m}) = f(\\theta^{m})$\n",
    "\n",
    "则，\n",
    "\n",
    "> + $g(\\theta^{m+1}|\\theta^{m}) = \\arg\\max_{\\theta} g(\\theta|\\theta^{m}) $\n",
    "\n",
    "因为，$f(\\theta^{m+1}) >= g(\\theta^{m+1}|\\theta^{m}) >= g(\\theta^{m}|\\theta^{m}) = f(\\theta^{m})$\n",
    "\n",
    "### 1.3.3. ELBO Evidence Lower BOund\n",
    "\n",
    "对于包含隐含变量$Z$的数据集$X$，根据概率公式\n",
    "\n",
    "> $p(X, Z|\\theta) = {p(Z|X, \\theta)} p(X|\\theta) $ \n",
    "\n",
    "$X$的对数似然为，\n",
    "> $\\ln p(X|\\theta) = \\ln \\left ( \\frac {p(X, Z|\\theta)}{p(Z|X, \\theta)} \\right ) $\n",
    "\n",
    "引入替代函数$q(Z)$，$\\sum_{Z}q(Z) = 1$，\n",
    "\n",
    "> $\\ln p(X|\\theta) = \\left(\\sum_{Z}q(Z)\\right)\n",
    "\\ln p(X|\\theta)$\n",
    "\n",
    "> $ = \\sum_{Z}q(Z) \\ln \\left (\n",
    " \\frac{p(X, Z|\\theta)}{p(Z|X, \\theta)} \\right ) $\n",
    "\n",
    "> $ = \\sum_{Z}q(Z) \\ln \\left (\n",
    " \\frac{p(X, Z|\\theta) / q(Z)}\n",
    " {p(Z|X, \\theta) / q(Z)} \\right )\n",
    "$\n",
    "\n",
    "> $ = \\sum_{Z}q(Z) \\left (\n",
    "\\ln \\frac{p(X, Z|\\theta)}{q(Z)} - \n",
    "\\ln \\frac{p(Z|X, \\theta) }{q(Z)}\n",
    "\\right)$\n",
    "\n",
    "> $ = \\sum_{Z}q(Z) \\ln \\frac{p(X, Z|\\theta)}{q(Z)} - \n",
    "\\sum_{Z}q(Z) \\ln \\frac{p(Z|X, \\theta) }{q(Z)}\n",
    "$\n",
    "\n",
    "> $ = \\underbrace{ \n",
    "\\sum_{Z}q(Z) \\ln \\frac{p(X, Z|\\theta)}{q(Z)}}\n",
    "_{\\mathcal{L}(q,\\theta)} + \n",
    "\\underbrace{\n",
    "\\sum_{Z}q(Z) \\ln \\frac{q(Z)}{p(Z|X, \\theta)}}\n",
    "_{\\mathbb{KL}(q(Z)||p(Z|X,\\theta))}\n",
    "$\n",
    "\n",
    "> $ = \\mathcal{L}(q,\\theta) + \n",
    "\\mathbb{KL}(q(Z)||p(Z|X,\\theta))\n",
    "$\n",
    "\n",
    "> $ >= \\mathcal{L}(q,\\theta)$\n",
    "\n",
    "所以，每次迭代可通过最大化$\\mathcal{L}(q,\\theta)$来最大化$\\ln p(X|\\theta)$。此时$\\mathbb{KL}(q(Z)||p(Z|X,\\theta))$取得最小值，即$q(Z) = p(Z|X,\\theta))$。\n",
    "\n",
    "选取$q(Z) = p(Z|X,\\theta^{old}))$，\n",
    "\n",
    "> $\\mathcal{L}(q,\\theta) = \\sum_{Z} p(Z|X,\\theta^{old}))\n",
    "\\ln \\frac{p(X, Z|\\theta)}{p(Z|X,\\theta^{old}))}$\n",
    "\n",
    "> $ = \\underbrace{\n",
    "\\sum_{Z} p(Z|X,\\theta^{old})) \\ln p(X, Z|\\theta)}_\n",
    "{\\mathcal{Q}(\\theta, \\theta^{old})} - \n",
    "\\underbrace{\n",
    "\\sum_{Z} p(Z|X,\\theta^{old})) p(Z|X,\\theta^{old}))}_\n",
    "{const}\n",
    "$\n",
    "\n",
    "> $ = \\mathcal{Q}(\\theta, \\theta^{old}) + const $\n",
    "\n",
    "然后最大化$\\mathcal{Q}(\\theta, \\theta^{old})$，即完整数据的对数似然$\\ln p(X, Z|\\theta)$的期望。\n",
    "\n",
    "\n",
    "# 2. 贝叶斯推断方法\n",
    "\n",
    "## 2.1. 最大后验估计 Maximum A Posterior\n",
    "\n",
    "MLE认为数据服从固定的参数决定的分布，贝叶斯学派认为参数是随机变量，本身也服从一定的分布。\n",
    "\n",
    "将参数的先验分布考虑进来。假设\n",
    "\n",
    "> $\\mu \\sim \\mathcal{N}(0,1)$，$\\sigma^2 \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "由于 $posterior \\propto likelihood \\times prior$，从而\n",
    "\n",
    "> $\\mu^{*}, {\\sigma}^{*} = \n",
    "\\arg\\max_{\\mu, \\sigma} \n",
    "\\{\\prod_{n=1}^{N} \\mathcal{N}(x_n; \\mu,\\sigma^2)\\} \n",
    "\\mathcal{N}(\\mu; 0,1) \n",
    "\\mathcal{N}(\\sigma^2; 0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2. 抽样方法 Sampling Methods\n",
    "\n",
    "对于包含隐变量的模型参数估计，隐变量的后验概率$p(Z|X, \\theta)$估计非常关键。\n",
    "\n",
    "如果（1）隐变量的维度过高；（2）或者隐变量后验概率的期望非常难以计算；则不能直接使用EM算法来精确计算模型的参数。需要采用近似推断的方式。包括：（1）采用随机近似的<b>抽样方法</b>；（2）采用确定近似的<b>变分推断</b>方法。\n",
    "\n",
    "\n",
    "## 2.3. 变分推断 Variational Inference\n",
    "\n",
    "主要目的：用易计算的近似分布替代不易计算的参数后验概率分布。\n",
    "\n",
    "主要问题：选择易计算的近似分布；尽可能替代原始的参数后验概率分布（KL散度）。\n",
    "\n",
    "\n",
    "### 2.3.1. Intractability\n",
    "\n",
    "（1）假设某个概率分布参数$\\boldsymbol{\\theta}$有500个，每一个有10,000个离散值，则需要计算500\\*10,000个数值才能得到该概率分布的积分；\n",
    "\n",
    "（2）常常是配分函数难以计算；\n",
    "\n",
    "> $p(z|x) = \\frac{p(z,x)}{p(x)} = \n",
    "\\frac{p(x|z)p(z)}{p(x)}$\n",
    "\n",
    "$p(x|z)$以及$p(z)$常常是用户自己选择的，易于计算。需要求解$p(x)$，并要求$\\sum_{z}p(z|x) = 1$；\n",
    "\n",
    "> $p(x) = \\sum_{z}p(z|x)p(z)$\n",
    "\n",
    "假设$z$有$M$维，每一个维度有$K$个状态，则需要遍历$K^{M}$个状态，才能计算得到$p(x)$。例如掷$M$个硬币，需要统计$2^{M}$个状态。\n",
    "\n",
    "### 2.3.2 I-projection & M-projection\n",
    "\n",
    "(1) <b>I-projection</b>\n",
    "\n",
    "> $\\mathbb{KL}(q||p) = \\sum_{x}q(x)\\ln \\frac{q(x)}{p(x)}$\n",
    "\n",
    "如果$p(x) = 0$，$q(x)$必须等于0，否则$\\mathbb{KL}(q||p)$变成无穷大。\n",
    "因此，I-projection是<b>zero-forcing</b>的，$q(x)$会低估(under-estimate)$p(x)$。\n",
    "\n",
    "(2) <b>M-projection</b>\n",
    "\n",
    "> $\\mathbb{KL}(p||q) = \\sum_{x}p(x)\\ln \\frac{p(x)}{q(x)}$\n",
    "\n",
    "如果$p(x) > 0$，$q(x)$必须大于0，则M-projection是<b>zero-avoiding</b>的，$q(x)$会过估(over-estimate)$p(x)$。\n",
    "\n",
    "I-projection会让$q(x)$趋近于一个单模态，用I-projection会让替代函数接近$p(x)$的局部分布。\n",
    "\n",
    "### 2.3.3. 变分推断 Variational Inference\n",
    "\n",
    "> $\\ln p(X) = \\mathcal{L}(q) + \n",
    "\\mathbb{KL}(q(Z)||p(Z|X))$\n",
    "\n",
    "> $ >= \\mathcal{L}(q) = \\sum_{Z}q(Z) \n",
    "\\ln \\frac{p(X,Z)}{q(Z)}$\n",
    "\n",
    "与EM略有不同的是，认为模型参数$\\theta$也是随机变量，从而可以与隐含变量$Z$一起计算。\n",
    "\n",
    "如果隐含变量的后验概率分布$p(Z|X)$难以计算，选用易于计算的替代函数$q(Z)$。\n",
    "\n",
    "最小化I-projection $\\mathbb{KL}(q||p)$等价于最大化$\\mathcal{L}(q)$，$\\mathbb{KL}(q||p)$不好优化，可以选择易于优化的$\\mathcal{L}(q)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. 平均场理论 Mean Field Theory\n",
    "\n",
    "假设替代函数$q(\\boldsymbol{Z})$可被划分为$K$个不相交的子集$\\boldsymbol{Z}_k$。\n",
    "\n",
    "> $q(\\boldsymbol{Z}) = \\prod_{k=1}^{K} q_k(\\boldsymbol{Z}_k) $\n",
    "\n",
    "我们需要最大化Lower Bound $\\mathcal{L}(q)$， \n",
    "\n",
    "> $\\mathcal{L}(q) = \\int q(\\boldsymbol{Z})\n",
    "\\ln \\frac{p(\\boldsymbol{X}, \\boldsymbol{Z})}{q(\\boldsymbol{Z})}\n",
    "d(\\boldsymbol{Z}) $\n",
    "\n",
    "考虑某个特定的子集$q_j(\\boldsymbol{Z}_j)$，与$q_j$无关的视作const，\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mathcal{L}(q_j) = \\int_{dom(1..K)}\n",
    "\\{ \\prod_{k=1}^{K}q_k \\}\n",
    "\\{ \\ln p(X, Z) - \\ln \\{ \\prod_{k=1}^{K}q_k \\} \\}\n",
    "dZ\n",
    "$\n",
    "\n",
    "> $ = \\int_{dom(1..K)} \\{ \\prod_{k=1}^{K}q_k \\} \n",
    "\\{ \\ln p(X, Z) \\} dZ - \n",
    "\\int_{dom(1..K)} \\{ \\prod_{i=1}^{K}q_i \\} \n",
    "\\{ \\sum_{k=1}^{K} \\ln q_k \\} dZ\n",
    "$\n",
    "\n",
    "> $ = \\int_{dom(j)} \\int_{dom(k \\neq j)} q_j \n",
    "\\prod_{k \\neq j} \\{q_k d Z_k\\}\n",
    "\\ln p(X, Z)\n",
    "d Z_j - \n",
    "\\sum_{k=1}^{K} \n",
    "\\int_{dom(1..K)} \\{ \\prod_{i=1}^{K}q_i \\}\n",
    "\\ln q_k dZ\n",
    "$\n",
    "\n",
    "> $ = \\int_{dom(j)} q_j \\{\n",
    "\\int_{dom(k \\neq j)}  \n",
    "\\ln p(X, Z)\\prod_{k \\neq j} \\{q_k d Z_k\\}\n",
    "\\} d Z_j -\n",
    "\\sum_{k=1}^{K} \\int_{dom(k)} \n",
    "\\int_{dom(i \\neq k)} q_k \\{ \\prod_{i \\neq k} \\left(q_i d Z_i \\right) \\} \\ln q_k d Z_k\n",
    "$\n",
    "\n",
    "> $ = \\int_{dom(j)} q_j \n",
    "\\mathbb{E}_{k \\neq j} (\\ln p(X, Z))\n",
    "d Z_j - \n",
    "\\sum_{k=1}^{K} \\int_{dom(k)} q_k \\ln q_k d Z_k\n",
    "\\prod_{i \\neq k} \\{ \n",
    "\\underbrace{\n",
    "\\int_{dom(i)} q_i d Z_i}_{\\int_{dom(i)} q_i d Z_i = 1}\n",
    "\\}\n",
    "$\n",
    "\n",
    "> $ = \\int_{dom(j)} q_j \n",
    "\\mathbb{E}_{k \\neq j} (\\ln p(X, Z))\n",
    "d Z_j - \n",
    "\\sum_{k=1}^{K} \\int_{dom(k)} q_k \\ln q_k d Z_k$\n",
    "\n",
    "> $ = \\int_{dom(j)} q_j \n",
    "\\mathbb{E}_{k \\neq j} (\\ln p(X, Z))\n",
    "d Z_j - \n",
    "\\int_{dom(j)} q_j \\ln q_j d Z_j + const$\n",
    "\n",
    "> $ = \\int_{dom(j)} q_j\n",
    "\\{ \\mathbb{E}_{k \\neq j} (\\ln p(X, Z)) - \\ln q_j \\}\n",
    "d Z_j + const $\n",
    "\n",
    "> $ = \\int_{dom(j)} q_j \\{ \n",
    "\\ln \\left( exp(\\mathbb{E}) \\right) - \\ln q_j \\} \n",
    "d Z_j + const $\n",
    "\n",
    "> $ = \\int_{dom(j)} q_j \n",
    "\\ln \\frac{exp(\\mathbb{E})}{q_j} d Z_j + const$\n",
    "\n",
    "> $ = - \\int_{dom(j)} q_j \n",
    "\\ln \\frac{q_j}{exp(\\mathbb{E})} d Z_j + const $\n",
    "\n",
    "> $ = - \\mathbb{KL}(exp(\\mathbb{E})||q_j) + const$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，\n",
    "\n",
    "> $\\mathcal{L}(q_j) = - \\mathbb{KL}(exp(\\mathbb{E})||q_j) + const$\n",
    "\n",
    "最大化$\\mathcal{L}(q_j)$等价于最小化$\\mathbb{KL}(exp(\\mathbb{E})||q_j)$；并且$q_j$是一个分布，$\\int q_j d Z_j = 1$；\n",
    "\n",
    "> $q_j = \\frac{1}{Z_j} exp\\{ \\mathbb{E}_{k \\neq j} (\\ln p(X, Z)) \\}$\n",
    "\n",
    "> $\\log q_j = \\mathbb{E}_{k \\neq j} (\\ln p(X, Z)) - \\log Z_j $\n",
    "\n",
    "> $ = \\mathbb{E}_{k \\neq j} (\\ln p(X, Z)) + const$\n",
    "\n",
    "### 2.3.4 随机梯度变分贝叶斯 Stochastic Gradient Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 变分自动编码机 Variational AutoEncoder\n",
    "\n",
    "参考资料：\n",
    "\n",
    "[1] Auto-Encoding Variational Bayes. \n",
    "\n",
    "[2] Tutorial on Variational AutoEncoders.\n",
    "\n",
    "# 3.1 设置目标函数\n",
    "\n",
    "考虑包含隐变量的生成模型，\n",
    "\n",
    "> $p(X) = \\int p(X|z;\\theta)p(z) dz$\n",
    "\n",
    "假设必须满足$p(X|z)$可以被计算，且对$\\theta$连续；例如：\n",
    "\n",
    "> $p(X|z;\\theta) \\sim \\mathcal{N}(X|f(z;\\theta), \\sigma^2 \\boldsymbol{I})$ \n",
    "\n",
    "> $p(z) \\sim \\mathcal{N}(z|\\boldsymbol{0}, \\boldsymbol{I})$\n",
    "\n",
    "如果$X$是二元变量，可以假设$p(X|z;\\theta) \\sim Bernoulli$。\n",
    "\n",
    "[?]实际中，对于大多数$z$，$p(X|z)$都比较接近于0，对于$p(X)$的估计基本上不起作用。需要使用替代函数$Q(z|X)$来根据$X$得到能够生成$X$的$z$，这样计算$\\mathbb{E}_{z \\sim Q}(p(X|z))$就变得容易很多。\n",
    "\n",
    "## 3.2 目标函数\n",
    "\n",
    "> $\\mathbb{KL}_{Q}(Q(z)||p(z|X)) = \\mathbb{E}_{z \\sim Q} [\\log Q(z) - \n",
    "\\underbrace{\n",
    "\\log p(z|X)}_\n",
    "{\\log \\frac{p(X|z)p(z)}{p(X)}}\n",
    "]\n",
    "$\n",
    "\n",
    "> $ = \\mathbb{E}_{z \\sim Q} [\\log Q(z) - \\log p(X|z) - \\log p(z)] + \n",
    "\\underbrace{\n",
    "\\log p(X)\n",
    "}_{与z无关}\n",
    "$\n",
    "\n",
    "> $J = \\log p(X) - \\mathbb{KL}_{Q}(Q(z)||p(z|X)) = \n",
    "\\mathbb{E}_{z \\sim Q} [ \\log p(X|z) ]  - \n",
    "\\mathbb{KL}(Q(z)||p(z))\n",
    "$\n",
    "\n",
    "实际上，只要能够很好地将$X$映射到$z$并且从$z$能生成$X$，$Q$可以是任何函数。但为了更好地对$X$进行推断，设置$Q$与$X$有关。\n",
    "\n",
    "> $J = \\underbrace{\n",
    "\\log p(X) - \\mathbb{KL}_{Q}(Q(z|X)||p(z|X))}_{(1)}\n",
    "= \n",
    "\\underbrace{\n",
    "\\mathbb{E}_{z \\sim Q} [ \\log p(X|z) ]  - \n",
    "\\mathbb{KL}(Q(z|X|)||p(z))}_{(2)}\n",
    "$\n",
    "\n",
    "从而，目标函数\n",
    "\n",
    "> $z^{*}, Q^{*} = \\arg\\max_{z, Q} J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 目标函数的优化\n",
    "\n",
    "目标函数中(1)并不好直接优化，选择优化(2)。需要确定$Q(z|X)$的形式，通常设置为高斯分布，\n",
    "\n",
    "> $Q(z|X) = \\mathcal{N}(z|\\mu(X;\\theta), \\Sigma(X;\\theta))$\n",
    "\n",
    "$\\mu$与$\\Sigma$可以是任意确定性的函数，$\\theta$从数据中学习得到。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
